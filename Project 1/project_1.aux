\relax 
\providecommand\hyper@newdestlabel[2]{}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wiki:Regression_analysis}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{FYS-STK4155 \IeC {\textendash } Applied data analysis and machine learning\\ Project 1 - Regression analysis and resampling methods}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {abstract}{Sammendrag}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}\protected@file@percent }
\newlabel{sec:Introduction}{{I}{1}{}{section*.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Theory}{1}{section*.4}\protected@file@percent }
\newlabel{sec:Theory}{{II}{1}{}{section*.4}{}}
\newlabel{eq:01}{{II.1}{1}{}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Moments and error analysis}{1}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Moments in statistics}{1}{section*.6}\protected@file@percent }
\citation{HastieTrevor2009TEoS}
\newlabel{eq:02}{{II.6}{2}{}{equation.2.6}{}}
\newlabel{eq:03}{{II.7}{2}{}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Error analysis}{2}{section*.7}\protected@file@percent }
\newlabel{eq:05}{{II.9}{2}{}{equation.2.9}{}}
\newlabel{eq:11}{{II.10}{2}{}{equation.2.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Linear regression methods}{2}{section*.8}\protected@file@percent }
\newlabel{eq:10}{{II.12}{2}{}{equation.2.12}{}}
\citation{MHJ_LinReg}
\citation{LayDavidC2016Laai}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Ordinary least squares}{3}{section*.9}\protected@file@percent }
\newlabel{eq:06}{{II.17}{3}{}{equation.2.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Ridge and Lasso regression}{3}{section*.10}\protected@file@percent }
\newlabel{eq:07}{{II.24}{3}{}{equation.2.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Confidence intervals}{3}{section*.11}\protected@file@percent }
\newlabel{eq:12}{{II.26}{3}{}{equation.2.26}{}}
\citation{MHJ_LinReg}
\citation{SVD}
\citation{SVD}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}SVD}{4}{section*.12}\protected@file@percent }
\newlabel{eq:08}{{II.28}{4}{}{equation.2.28}{}}
\newlabel{eq:09}{{II.29}{4}{}{equation.2.29}{}}
\newlabel{eq:13}{{II.30}{4}{}{equation.2.30}{}}
\newlabel{eq:14}{{II.31}{4}{}{equation.2.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Resampling and Bias-variance tradeoff}{4}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Reasmpling methods}{4}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Bias-variance tradeoff}{4}{section*.15}\protected@file@percent }
\newlabel{eq:bias}{{II.32}{5}{}{equation.2.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{5}{section*.16}\protected@file@percent }
\newlabel{sec:Method}{{III}{5}{}{section*.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Preparations}{5}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Regression analysis}{6}{section*.18}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces A illustration of how the error estimate can differ depending on the fold and the \(\lambda \) value. The ideal \(\lambda \) would here be \(\lambda _4\).\relax }}{6}{table.caption.19}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:01}{{I}{6}{A illustration of how the error estimate can differ depending on the fold and the \(\lambda \) value. The ideal \(\lambda \) would here be \(\lambda _4\).\relax }{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Franke function}{6}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Terrain data}{7}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{8}{section*.22}\protected@file@percent }
\newlabel{sec:Results}{{IV}{8}{}{section*.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Franke function}{8}{section*.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The Franke function with added Gaussin noise with \(\mu =0\) and \(\sigma =1\) for a grid of \(81\times 81\) randomly distributed data points taken from a uniform distribution between \([0,1]\).\relax }}{8}{figure.caption.24}\protected@file@percent }
\newlabel{fig:Franke_w_noise}{{1}{8}{The Franke function with added Gaussin noise with \(\mu =0\) and \(\sigma =1\) for a grid of \(81\times 81\) randomly distributed data points taken from a uniform distribution between \([0,1]\).\relax }{figure.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The \(\beta \)-values for the OLS, Ridge and Lasso regression methods with their respective confidence intervals. For Ridge \(\lambda = 4.95\cdot 10^{-3}\) while for Lasso \(\lambda = 6.34\cdot 10^{-5}\).\relax }}{9}{table.caption.25}\protected@file@percent }
\newlabel{tab:02}{{II}{9}{The \(\beta \)-values for the OLS, Ridge and Lasso regression methods with their respective confidence intervals. For Ridge \(\lambda = 4.95\cdot 10^{-3}\) while for Lasso \(\lambda = 6.34\cdot 10^{-5}\).\relax }{table.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref  {tab:01}. The error estimates are calculated both with respect to the actual Franke function and the data set used to create the model.\relax }}{9}{table.caption.26}\protected@file@percent }
\newlabel{tab:03}{{III}{9}{The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref {tab:01}. The error estimates are calculated both with respect to the actual Franke function and the data set used to create the model.\relax }{table.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref  {tab:13}. The error estimates are for the training section of the data and are calculated both with respect to the actual Franke function and the data set used to create the model.\relax }}{9}{table.caption.27}\protected@file@percent }
\newlabel{tab:04}{{IV}{9}{The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref {tab:13}. The error estimates are for the training section of the data and are calculated both with respect to the actual Franke function and the data set used to create the model.\relax }{table.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref  {tab:13}. The error estimates are for the test section of the data and are calculated both with respect to the actual Franke function and the test section of the data set used to create the model.\relax }}{9}{table.caption.28}\protected@file@percent }
\newlabel{tab:05}{{V}{9}{The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref {tab:13}. The error estimates are for the test section of the data and are calculated both with respect to the actual Franke function and the test section of the data set used to create the model.\relax }{table.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda \) values. The \(\lambda \) which gives the minimum MSE for a 5\(^\text  {th}\) order polynomial is highlighted.\relax }}{10}{figure.caption.29}\protected@file@percent }
\newlabel{fig:Vary_lambda_R}{{2}{10}{The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda \) values. The \(\lambda \) which gives the minimum MSE for a 5\(^\text {th}\) order polynomial is highlighted.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The MSE between the test set and the predicted model using Lasso regression for different polynomial orders and \(\lambda \) values. The \(\lambda \) which gives the minimum MSE for a 5\(^\text  {th}\) order polynomial is highlighted. The model is calculated using 1001 iterations pr \(\lambda \)-value.\relax }}{10}{figure.caption.30}\protected@file@percent }
\newlabel{fig:Vary_lambda_L}{{3}{10}{The MSE between the test set and the predicted model using Lasso regression for different polynomial orders and \(\lambda \) values. The \(\lambda \) which gives the minimum MSE for a 5\(^\text {th}\) order polynomial is highlighted. The model is calculated using 1001 iterations pr \(\lambda \)-value.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using OLS.\relax }}{10}{figure.caption.31}\protected@file@percent }
\newlabel{fig:K_fold10_OLS}{{4}{10}{The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using OLS.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using Ridge with the \(\lambda \)-values given in the plot.\relax }}{11}{figure.caption.32}\protected@file@percent }
\newlabel{fig:K_fold10_Ridge}{{5}{11}{The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using Ridge with the \(\lambda \)-values given in the plot.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using Lasso with the \(\lambda \)-values given in the plot.\relax }}{11}{figure.caption.33}\protected@file@percent }
\newlabel{fig:K_fold10_Lasso}{{6}{11}{The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using Lasso with the \(\lambda \)-values given in the plot.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A plot of the MSE from the test and training folds in k-fold cross validation with \(k=5\) folds for the Franke data using OLS. The values are the average of \(N=50\) runs with randomized folds.\relax }}{11}{figure.caption.34}\protected@file@percent }
\newlabel{fig:test_vs_train}{{7}{11}{A plot of the MSE from the test and training folds in k-fold cross validation with \(k=5\) folds for the Franke data using OLS. The values are the average of \(N=50\) runs with randomized folds.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A plot of the R\(^2\) score from the test and training folds in k-fold cross validation with \(k=5\) folds for the Franke data. The values are the average of \(N=50\) runs with randomized folds.\relax }}{12}{figure.caption.35}\protected@file@percent }
\newlabel{fig:test_vs_trainR2}{{8}{12}{A plot of the R\(^2\) score from the test and training folds in k-fold cross validation with \(k=5\) folds for the Franke data. The values are the average of \(N=50\) runs with randomized folds.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A plot of the Bias\(^2\) and Variance for the Franke data by re randomizing the noise in the data set while keeping the data points constant. The Bias\(^2\) and Variance is calculated based on the models fitted on the test section.\relax }}{12}{figure.caption.36}\protected@file@percent }
\newlabel{fig:bias_varianceOLS}{{9}{12}{A plot of the Bias\(^2\) and Variance for the Franke data by re randomizing the noise in the data set while keeping the data points constant. The Bias\(^2\) and Variance is calculated based on the models fitted on the test section.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The R\(^2\) between a test set from the data set and the regression methods OLS, ridge and lasso using the ''optimal'' \(\lambda \)-values for each polynomial degree and method from figure \ref  {fig:optimal_lambdas}.\relax }}{12}{figure.caption.37}\protected@file@percent }
\newlabel{fig:optimal_r2_franke}{{10}{12}{The R\(^2\) between a test set from the data set and the regression methods OLS, ridge and lasso using the ''optimal'' \(\lambda \)-values for each polynomial degree and method from figure \ref {fig:optimal_lambdas}.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The R\(^2\) score between a test set and the regression methods OLS, ridge and lasso using the ''optimal'' \(\lambda \)-values for each polynomial degree and method from figure \ref  {fig:optimal_lambdas}. The test set here is for the real Franke function without noise, but it correspond to the part of the data set omitted in training the model.\relax }}{13}{figure.caption.38}\protected@file@percent }
\newlabel{fig:optimal_r2_data}{{11}{13}{The R\(^2\) score between a test set and the regression methods OLS, ridge and lasso using the ''optimal'' \(\lambda \)-values for each polynomial degree and method from figure \ref {fig:optimal_lambdas}. The test set here is for the real Franke function without noise, but it correspond to the part of the data set omitted in training the model.\relax }{figure.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces The lowest MSE and highest R\(^2\) estimate with corresponding polynomail degree from figure \ref  {fig:optimal_r2_franke}, \ref  {fig:optimal_r2_data}, \ref  {fig:optimal_mse_franke} and \ref  {fig:optimal_mse_data}. The \(\lambda \)-values used for ridge is \(\lambda = 0.002458\) and \(\lambda = 0.03091\) and for lasso \(\lambda = 7.2605\cdot 10^{-6}\) and \(\lambda = 3.7727\cdot 10^{-5}\) for Franke and data respectively.\relax }}{13}{table.caption.39}\protected@file@percent }
\newlabel{tab:06}{{VI}{13}{The lowest MSE and highest R\(^2\) estimate with corresponding polynomail degree from figure \ref {fig:optimal_r2_franke}, \ref {fig:optimal_r2_data}, \ref {fig:optimal_mse_franke} and \ref {fig:optimal_mse_data}. The \(\lambda \)-values used for ridge is \(\lambda = 0.002458\) and \(\lambda = 0.03091\) and for lasso \(\lambda = 7.2605\cdot 10^{-6}\) and \(\lambda = 3.7727\cdot 10^{-5}\) for Franke and data respectively.\relax }{table.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces The MSE and R\(^2\) scores for the entire data set with the best performing models from table \ref  {tab:06} for OLS, ridge and lasso regression.\relax }}{13}{table.caption.40}\protected@file@percent }
\newlabel{tab:07}{{VII}{13}{The MSE and R\(^2\) scores for the entire data set with the best performing models from table \ref {tab:06} for OLS, ridge and lasso regression.\relax }{table.caption.40}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces The MSE and R\(^2\) scores for the entire data using the models with best performance in the test data errors from table \ref  {tab:07} for OLS, ridge and lasso regression. The models are here used on the entire data grid not just the test or training section.\relax }}{13}{table.caption.41}\protected@file@percent }
\newlabel{tab:07_2}{{VIII}{13}{The MSE and R\(^2\) scores for the entire data using the models with best performance in the test data errors from table \ref {tab:07} for OLS, ridge and lasso regression. The models are here used on the entire data grid not just the test or training section.\relax }{table.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces A 3D plot of the 13\(^\text  {th}\) order Ridge model with \(\lambda = 0.03091\) and the Franke function.\relax }}{14}{figure.caption.42}\protected@file@percent }
\newlabel{fig:ridge_3d}{{12}{14}{A 3D plot of the 13\(^\text {th}\) order Ridge model with \(\lambda = 0.03091\) and the Franke function.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces A plot of the MSE from the test and training folds in k-fold cross validation with \(k=5\) folds for the Franke data using OLS. The values are the average of \(N=50\) runs with randomized folds. Unlike figure \ref  {fig:test_vs_train} the data-set here is of size \(400\times 200\).\relax }}{14}{figure.caption.43}\protected@file@percent }
\newlabel{fig:test_vs_trainlarge_n}{{13}{14}{A plot of the MSE from the test and training folds in k-fold cross validation with \(k=5\) folds for the Franke data using OLS. The values are the average of \(N=50\) runs with randomized folds. Unlike figure \ref {fig:test_vs_train} the data-set here is of size \(400\times 200\).\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Terrain data}{14}{section*.44}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Colormesh plot of the terrain data used from file ``SRTM\_data\_Norway\_2" after being resampled to a size of \(400\times 200\).\relax }}{14}{figure.caption.45}\protected@file@percent }
\newlabel{fig:terrain_data}{{14}{14}{Colormesh plot of the terrain data used from file ``SRTM\_data\_Norway\_2" after being resampled to a size of \(400\times 200\).\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces A plot of the MSE from the test and training folds in k-fold cross validation with \(k=5\) folds for the terrain data using OLS. The values are the average of \(N=10\) runs with randomized folds.\relax }}{15}{figure.caption.46}\protected@file@percent }
\newlabel{fig:test_vs_train2}{{15}{15}{A plot of the MSE from the test and training folds in k-fold cross validation with \(k=5\) folds for the terrain data using OLS. The values are the average of \(N=10\) runs with randomized folds.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces A plot of the R\(^2\) score from the test and training folds in k-fold cross validation with \(k=5\) folds for the terrain data using OLS. The values are the average of \(N=10\) runs with randomized folds.\relax }}{15}{figure.caption.47}\protected@file@percent }
\newlabel{fig:test_vs_train3}{{16}{15}{A plot of the R\(^2\) score from the test and training folds in k-fold cross validation with \(k=5\) folds for the terrain data using OLS. The values are the average of \(N=10\) runs with randomized folds.\relax }{figure.caption.47}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IX}{\ignorespaces The error estimates MSE and R\(^2\) from figure \ref  {fig:test_vs_train2} and \ref  {fig:test_vs_train3} for 15\(^\text  {th}\) degree polynomial.\relax }}{16}{table.caption.48}\protected@file@percent }
\newlabel{tab:08}{{IX}{16}{The error estimates MSE and R\(^2\) from figure \ref {fig:test_vs_train2} and \ref {fig:test_vs_train3} for 15\(^\text {th}\) degree polynomial.\relax }{table.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces A plot of the mean MSE as a function of \(\lambda \) for the test folds in k-fold cross validation for \(k=5\) folds using ridge regression with the terrain data, with \(\lambda \in [10^{-9}, 10^{-7}]\) logarithmic spaced.\relax }}{16}{figure.caption.49}\protected@file@percent }
\newlabel{fig:ridge_bad}{{17}{16}{A plot of the mean MSE as a function of \(\lambda \) for the test folds in k-fold cross validation for \(k=5\) folds using ridge regression with the terrain data, with \(\lambda \in [10^{-9}, 10^{-7}]\) logarithmic spaced.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces A plot of the mean MSE as a function of \(\lambda \) for the test folds in k-fold cross validation for \(k=5\) folds using lasso regression with the terrain data, with \(\lambda \in [10^{-11}, 10^{-9}]\) logarithmic spaced.\relax }}{16}{figure.caption.50}\protected@file@percent }
\newlabel{fig:lasso_bad}{{18}{16}{A plot of the mean MSE as a function of \(\lambda \) for the test folds in k-fold cross validation for \(k=5\) folds using lasso regression with the terrain data, with \(\lambda \in [10^{-11}, 10^{-9}]\) logarithmic spaced.\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The \(\lambda \)-values which minimizes the mean of the test errors in k-fold cross validation with 4-folds using ridge regression. The plotted \(\lambda \)-values are the mean of N = 50 k-fold runs with randomized folds pr complexity.\relax }}{16}{figure.caption.51}\protected@file@percent }
\newlabel{fig:lambda_pr_degree_terrain}{{19}{16}{The \(\lambda \)-values which minimizes the mean of the test errors in k-fold cross validation with 4-folds using ridge regression. The plotted \(\lambda \)-values are the mean of N = 50 k-fold runs with randomized folds pr complexity.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces A plot of the MSE from the test and training folds in k-fold cross validation with \(k=5\) folds for the terrain data using Ridge regression with the optimal \(\lambda \) values from figure \ref  {fig:lambda_pr_degree_terrain}. The values are the average of \(N=10\) runs with randomized folds.\relax }}{17}{figure.caption.52}\protected@file@percent }
\newlabel{fig:test_vs_train2ridge}{{20}{17}{A plot of the MSE from the test and training folds in k-fold cross validation with \(k=5\) folds for the terrain data using Ridge regression with the optimal \(\lambda \) values from figure \ref {fig:lambda_pr_degree_terrain}. The values are the average of \(N=10\) runs with randomized folds.\relax }{figure.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {X}{\ignorespaces The error estimates for the MSE in the test data from figure\ref  {fig:test_vs_train2} and \ref  {fig:test_vs_train2ridge} for all polynomials from 1 to 20.\relax }}{17}{table.caption.53}\protected@file@percent }
\newlabel{tab:09}{{X}{17}{The error estimates for the MSE in the test data from figure\ref {fig:test_vs_train2} and \ref {fig:test_vs_train2ridge} for all polynomials from 1 to 20.\relax }{table.caption.53}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XI}{\ignorespaces The error estimates MSE and R\(^2\) from figure \ref  {fig:test_vs_train2} and \ref  {fig:test_vs_train3} for 15\(^\text  {th}\) degree polynomial.\relax }}{17}{table.caption.54}\protected@file@percent }
\newlabel{tab:10}{{XI}{17}{The error estimates MSE and R\(^2\) from figure \ref {fig:test_vs_train2} and \ref {fig:test_vs_train3} for 15\(^\text {th}\) degree polynomial.\relax }{table.caption.54}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XII}{\ignorespaces The error estimates MSE and R\(^2\) for a OLS model created using the entire terrain data set with a 15\(^\text  {th}\) degree complexity. The \(\lambda \)-values used for ridge and lasso regression is \(\lambda = 10^{-8}\) for both.\relax }}{18}{table.caption.55}\protected@file@percent }
\newlabel{tab:11}{{XII}{18}{The error estimates MSE and R\(^2\) for a OLS model created using the entire terrain data set with a 15\(^\text {th}\) degree complexity. The \(\lambda \)-values used for ridge and lasso regression is \(\lambda = 10^{-8}\) for both.\relax }{table.caption.55}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XIII}{\ignorespaces The mean error estimates MSE and R\(^2\) for the test data with OLS, ridge and lasso using k-fold cross validation with \(k=10\) folds. The \(\lambda \)-values used for ridge and lasso regression is \(\lambda = 10^{-8}\) for both. The error shown is the standard deviation with a confidence of \(95\%\).\relax }}{18}{table.caption.56}\protected@file@percent }
\newlabel{tab:12}{{XIII}{18}{The mean error estimates MSE and R\(^2\) for the test data with OLS, ridge and lasso using k-fold cross validation with \(k=10\) folds. The \(\lambda \)-values used for ridge and lasso regression is \(\lambda = 10^{-8}\) for both. The error shown is the standard deviation with a confidence of \(95\%\).\relax }{table.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces The MSE for the test folds in k-fold cross validation with \(k=10\) folds using OLS.\relax }}{18}{figure.caption.57}\protected@file@percent }
\newlabel{fig:hist_terrain_ols}{{21}{18}{The MSE for the test folds in k-fold cross validation with \(k=10\) folds using OLS.\relax }{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces A colormap of the model created using 15\(^\text  {th}\) degree polynomial complexity with OLS on the entire data set.\relax }}{18}{figure.caption.58}\protected@file@percent }
\newlabel{fig:colormap_terrain_model}{{22}{18}{A colormap of the model created using 15\(^\text {th}\) degree polynomial complexity with OLS on the entire data set.\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces A colormap of the model created using 5\(^\text  {th}\) degree polynomial complexity with OLS on the entire data set.\relax }}{18}{figure.caption.59}\protected@file@percent }
\newlabel{fig:colormap_terrain_model5th}{{23}{18}{A colormap of the model created using 5\(^\text {th}\) degree polynomial complexity with OLS on the entire data set.\relax }{figure.caption.59}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{19}{section*.60}\protected@file@percent }
\newlabel{sec:Discussion}{{V}{19}{}{section*.60}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Regression methods}{19}{section*.61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Data sets}{19}{section*.62}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Bias-variance tradeoff}{20}{section*.63}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{21}{section*.64}\protected@file@percent }
\newlabel{sec:Conclusion}{{VI}{21}{}{section*.64}{}}
\@writefile{toc}{\appendix }
\@writefile{toc}{\contentsline {section}{\numberline {A}Tables}{21}{section*.65}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {XIV}{\ignorespaces The \(\beta \)-values for the OLS, Ridge and Lasso regression models on the training data with their respective confidence interval at \(95\%\). For Ridge \(\lambda = 4.95\cdot 10^{-3}\) while for Lasso \(\lambda = 6.34\cdot 10^{-5}\).\relax }}{21}{table.caption.66}\protected@file@percent }
\newlabel{tab:13}{{XIV}{21}{The \(\beta \)-values for the OLS, Ridge and Lasso regression models on the training data with their respective confidence interval at \(95\%\). For Ridge \(\lambda = 4.95\cdot 10^{-3}\) while for Lasso \(\lambda = 6.34\cdot 10^{-5}\).\relax }{table.caption.66}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XV}{\ignorespaces The \(\beta \)-values for the OLS regression method using 10 folds in k-fold cross validation on the training data in a train/test data set. The confidence interval is 95\%, and is standard error between the corresponding \(\beta _i\)-coefficients i.e the std for \(\beta _0\), \(\beta _1\), \(\beta _2\) etc.\relax }}{22}{table.caption.67}\protected@file@percent }
\newlabel{tab:14}{{XV}{22}{The \(\beta \)-values for the OLS regression method using 10 folds in k-fold cross validation on the training data in a train/test data set. The confidence interval is 95\%, and is standard error between the corresponding \(\beta _i\)-coefficients i.e the std for \(\beta _0\), \(\beta _1\), \(\beta _2\) etc.\relax }{table.caption.67}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Figures}{23}{section*.68}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda \) values. It's the same plot as figure \ref  {fig:Vary_lambda_R} but for larger \(\lambda \) values i.e \(\lambda \in [0, 10]\) logarithmic spaced.\relax }}{23}{figure.caption.69}\protected@file@percent }
\newlabel{fig:Vary_lambda_R2}{{24}{23}{The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda \) values. It's the same plot as figure \ref {fig:Vary_lambda_R} but for larger \(\lambda \) values i.e \(\lambda \in [0, 10]\) logarithmic spaced.\relax }{figure.caption.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda \) values. It's the same plot as figure \ref  {fig:Vary_lambda_L} but for larger \(\lambda \) values i.e \(\lambda \in [0, 0.1]\) logarithmic spaced with 1001 iterations.\relax }}{23}{figure.caption.70}\protected@file@percent }
\newlabel{fig:Vary_lambda_L2}{{25}{23}{The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda \) values. It's the same plot as figure \ref {fig:Vary_lambda_L} but for larger \(\lambda \) values i.e \(\lambda \in [0, 0.1]\) logarithmic spaced with 1001 iterations.\relax }{figure.caption.70}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces The MSE between a test set from the data and the regression methods OLS, ridge and lasso using the ''optimal'' \(\lambda \)-values for each polynomial degree and method from figure \ref  {fig:optimal_lambdas}.\relax }}{23}{figure.caption.71}\protected@file@percent }
\newlabel{fig:optimal_mse_franke}{{26}{23}{The MSE between a test set from the data and the regression methods OLS, ridge and lasso using the ''optimal'' \(\lambda \)-values for each polynomial degree and method from figure \ref {fig:optimal_lambdas}.\relax }{figure.caption.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces The MSE between a test set and the regression methods OLS, ridge and lasso using the ''optimal'' \(\lambda \)-values for each polynomial degree and method from figure \ref  {fig:optimal_lambdas}. The test set here is for the real Franke function without noise, but it correspond to the part of the data set omitted in training the model.\relax }}{23}{figure.caption.72}\protected@file@percent }
\newlabel{fig:optimal_mse_data}{{27}{23}{The MSE between a test set and the regression methods OLS, ridge and lasso using the ''optimal'' \(\lambda \)-values for each polynomial degree and method from figure \ref {fig:optimal_lambdas}. The test set here is for the real Franke function without noise, but it correspond to the part of the data set omitted in training the model.\relax }{figure.caption.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces The ideal \(\lambda \) values for the Franke function.\relax }}{24}{figure.caption.73}\protected@file@percent }
\newlabel{fig:optimal_lambdas}{{28}{24}{The ideal \(\lambda \) values for the Franke function.\relax }{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces A 3D plot of the 4\(^\text  {th}\) order OLS model from table \ref  {tab:07} beside the Franke function.\relax }}{24}{figure.caption.74}\protected@file@percent }
\newlabel{fig:ols_3d}{{29}{24}{A 3D plot of the 4\(^\text {th}\) order OLS model from table \ref {tab:07} beside the Franke function.\relax }{figure.caption.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces A 3D plot of the 15\(^\text  {th}\) order Lasso model from table \ref  {tab:07} with \(\lambda = 3.7727\cdot 10^{-5}\) beside the Franke function.\relax }}{24}{figure.caption.75}\protected@file@percent }
\newlabel{fig:lasso_3d}{{30}{24}{A 3D plot of the 15\(^\text {th}\) order Lasso model from table \ref {tab:07} with \(\lambda = 3.7727\cdot 10^{-5}\) beside the Franke function.\relax }{figure.caption.75}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {a}Terrain}{24}{section*.76}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces The MSE for the test folds in k-fold cross validation with \(k=10\) folds for the terrain data and two different \(\lambda \)-values in ridge regression.\relax }}{24}{figure.caption.77}\protected@file@percent }
\newlabel{fig:hist_terrain_ridge}{{31}{24}{The MSE for the test folds in k-fold cross validation with \(k=10\) folds for the terrain data and two different \(\lambda \)-values in ridge regression.\relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces The MSE for the test folds in k-fold cross validation with \(k=10\) folds for the terrain data and two different \(\lambda \)-values in lasso regression.\relax }}{25}{figure.caption.78}\protected@file@percent }
\newlabel{fig:hist_terrain_lasso}{{32}{25}{The MSE for the test folds in k-fold cross validation with \(k=10\) folds for the terrain data and two different \(\lambda \)-values in lasso regression.\relax }{figure.caption.78}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Proofs}{25}{section*.79}\protected@file@percent }
\newlabel{sec:proof}{{C}{25}{}{section*.79}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {a}Bias-variance tradeoff proof}{25}{section*.80}\protected@file@percent }
\bibdata{project_1Notes,references}
\bibcite{HastieTrevor2009TEoS}{{1}{}{{}}{{}}}
\bibcite{MHJ_LinReg}{{2}{}{{}}{{}}}
\bibcite{LayDavidC2016Laai}{{3}{}{{}}{{}}}
\bibcite{wiki:Regression_analysis}{{4}{}{{}}{{}}}
\bibstyle{plain}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {}Referanser}{26}{section*.81}\protected@file@percent }
\newlabel{LastBibItem}{{4}{26}{}{section*.81}{}}
\newlabel{LastPage}{{}{26}{}{}{}}
