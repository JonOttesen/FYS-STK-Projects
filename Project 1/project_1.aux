\relax 
\providecommand\hyper@newdestlabel[2]{}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wiki:Regression_analysis}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{FYS-STK4155 \IeC {\textendash } Applied data analysis and machine learning\\ Project 1 - Regression analysis and resampling methods}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {abstract}{Sammendrag}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}\protected@file@percent }
\newlabel{sec:Introduction}{{I}{1}{}{section*.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Theory}{1}{section*.4}\protected@file@percent }
\newlabel{sec:Theory}{{II}{1}{}{section*.4}{}}
\newlabel{eq:01}{{II.1}{1}{}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Moments and error analysis}{1}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Moments in statistics}{1}{section*.6}\protected@file@percent }
\newlabel{eq:02}{{II.6}{2}{}{equation.2.6}{}}
\newlabel{eq:03}{{II.7}{2}{}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Error analysis}{2}{section*.7}\protected@file@percent }
\newlabel{eq:05}{{II.9}{2}{}{equation.2.9}{}}
\newlabel{eq:11}{{II.10}{2}{}{equation.2.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Linear regression methods}{2}{section*.8}\protected@file@percent }
\newlabel{eq:10}{{II.12}{2}{}{equation.2.12}{}}
\citation{MHJ_LinReg}
\citation{LayDavidC2016Laai}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Ordinary least squares}{3}{section*.9}\protected@file@percent }
\newlabel{eq:06}{{II.17}{3}{}{equation.2.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Ridge and Lasso regression}{3}{section*.10}\protected@file@percent }
\newlabel{eq:07}{{II.24}{3}{}{equation.2.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Confidence intervals}{3}{section*.11}\protected@file@percent }
\newlabel{eq:12}{{II.26}{3}{}{equation.2.26}{}}
\citation{MHJ_LinReg}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}SVD}{4}{section*.12}\protected@file@percent }
\newlabel{eq:08}{{II.28}{4}{}{equation.2.28}{}}
\newlabel{eq:09}{{II.29}{4}{}{equation.2.29}{}}
\newlabel{eq:13}{{II.30}{4}{}{equation.2.30}{}}
\newlabel{eq:14}{{II.31}{4}{}{equation.2.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Resampling and Bias-variance tradeoff}{4}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Reasmpling methods}{4}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Bias-variance tradeoff}{4}{section*.15}\protected@file@percent }
\newlabel{eq:bias}{{II.32}{5}{}{equation.2.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{5}{section*.16}\protected@file@percent }
\newlabel{sec:Method}{{III}{5}{}{section*.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Preparations}{5}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Regression analysis}{6}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Franke function}{6}{section*.20}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces A illustration of how the error estimate can differ depending on the fold and the \(\lambda \) value. The ideal \(\lambda \) would here be \(\lambda _4\).\relax }}{6}{table.caption.19}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:01}{{I}{6}{A illustration of how the error estimate can differ depending on the fold and the \(\lambda \) value. The ideal \(\lambda \) would here be \(\lambda _4\).\relax }{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Terrain data}{7}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{7}{section*.22}\protected@file@percent }
\newlabel{sec:Results}{{IV}{7}{}{section*.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Franke function}{7}{section*.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The Franke function with added Gaussin noise with \(\mu =0\) and \(\sigma =1\) for a grid of \(81\times 81\) randomly distributed data points taken from a uniform distribution between \([0,1]\).\relax }}{8}{figure.caption.24}\protected@file@percent }
\newlabel{fig:Franke_w_noise}{{1}{8}{The Franke function with added Gaussin noise with \(\mu =0\) and \(\sigma =1\) for a grid of \(81\times 81\) randomly distributed data points taken from a uniform distribution between \([0,1]\).\relax }{figure.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The \(\beta \)-values for the OLS, Ridge and Lasso regression methods with their respective confidence interval. For Ridge \(\lambda = 4.95\cdot 10^{-3}\) while for Lasso \(\lambda = 3.66\cdot 10^{-5}\).\relax }}{8}{table.caption.25}\protected@file@percent }
\newlabel{tab:02}{{II}{8}{The \(\beta \)-values for the OLS, Ridge and Lasso regression methods with their respective confidence interval. For Ridge \(\lambda = 4.95\cdot 10^{-3}\) while for Lasso \(\lambda = 3.66\cdot 10^{-5}\).\relax }{table.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref  {tab:01}. The error estimates are calculated both with respect to the actual Franke function and the data set used to create the model.\relax }}{8}{table.caption.26}\protected@file@percent }
\newlabel{tab:03}{{III}{8}{The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref {tab:01}. The error estimates are calculated both with respect to the actual Franke function and the data set used to create the model.\relax }{table.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref  {tab:13}. The error estimates are for the training section of the data and are calculated both with respect to the actual Franke function and the data set used to create the model.\relax }}{9}{table.caption.27}\protected@file@percent }
\newlabel{tab:04}{{IV}{9}{The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref {tab:13}. The error estimates are for the training section of the data and are calculated both with respect to the actual Franke function and the data set used to create the model.\relax }{table.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref  {tab:13}. The error estimates are for the test section of the data and are calculated both with respect to the actual Franke function and the test section of the data set used to create the model.\relax }}{9}{table.caption.28}\protected@file@percent }
\newlabel{tab:05}{{V}{9}{The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref {tab:13}. The error estimates are for the test section of the data and are calculated both with respect to the actual Franke function and the test section of the data set used to create the model.\relax }{table.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda \) values. The \(\lambda \) which gives the minimum MSE for a 5\(^\text  {th}\) order polynomial is highlighted.\relax }}{9}{figure.caption.29}\protected@file@percent }
\newlabel{fig:Vary_lambda_R}{{2}{9}{The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda \) values. The \(\lambda \) which gives the minimum MSE for a 5\(^\text {th}\) order polynomial is highlighted.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The MSE between the test set and the predicted model using Lasso regression for different polynomial orders and \(\lambda \) values. The \(\lambda \) which gives the minimum MSE for a 5\(^\text  {th}\) order polynomial is highlighted. The model is calculated using 2000 iterations pr \(\lambda \)-value.\relax }}{9}{figure.caption.30}\protected@file@percent }
\newlabel{fig:Vary_lambda_L}{{3}{9}{The MSE between the test set and the predicted model using Lasso regression for different polynomial orders and \(\lambda \) values. The \(\lambda \) which gives the minimum MSE for a 5\(^\text {th}\) order polynomial is highlighted. The model is calculated using 2000 iterations pr \(\lambda \)-value.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using OLS.\relax }}{10}{figure.caption.31}\protected@file@percent }
\newlabel{fig:K_fold10_OLS}{{4}{10}{The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using OLS.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using Ridge with the \(\lambda \)-values given in the plot.\relax }}{10}{figure.caption.32}\protected@file@percent }
\newlabel{fig:K_fold10_Ridge}{{5}{10}{The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using Ridge with the \(\lambda \)-values given in the plot.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using Lasso with the \(\lambda \)-values given in the plot.\relax }}{10}{figure.caption.33}\protected@file@percent }
\newlabel{fig:K_fold10_Lasso}{{6}{10}{The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using Lasso with the \(\lambda \)-values given in the plot.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The MSE between a OLS model for the Franke data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order. This is done \(N=50\) times with random folds and the plot shows the average.\relax }}{11}{figure.caption.34}\protected@file@percent }
\newlabel{fig:test_vs_train}{{7}{11}{The MSE between a OLS model for the Franke data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order. This is done \(N=50\) times with random folds and the plot shows the average.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The R\(^2\) score between a OLS model for the Franke data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order. This is done \(N=50\) times with random folds and the plot shows the average.\relax }}{11}{figure.caption.35}\protected@file@percent }
\newlabel{fig:test_vs_trainR2}{{8}{11}{The R\(^2\) score between a OLS model for the Franke data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order. This is done \(N=50\) times with random folds and the plot shows the average.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The R\(^2\) score between a OLS model for the Franke data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order. This is done \(N=50\) times with random folds and the plot shows the average.\relax }}{11}{figure.caption.36}\protected@file@percent }
\newlabel{fig:bias_varianceOLS}{{9}{11}{The R\(^2\) score between a OLS model for the Franke data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order. This is done \(N=50\) times with random folds and the plot shows the average.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The R\(^2\) between a test set from the data set and the regression method OLS, ridge and lasso model using ''optimal'' \(\lambda \)-parameters.\relax }}{12}{figure.caption.37}\protected@file@percent }
\newlabel{fig:optimal_r2_franke}{{10}{12}{The R\(^2\) between a test set from the data set and the regression method OLS, ridge and lasso model using ''optimal'' \(\lambda \)-parameters.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The R\(^2\) between a test set and the regression method OLS, ridge and lasso model using ''optimal'' \(\lambda \)-parameters. The test set here is for the real Franke function, but it correspond to the part of the data set omitted in the training data.\relax }}{12}{figure.caption.38}\protected@file@percent }
\newlabel{fig:optimal_r2_data}{{11}{12}{The R\(^2\) between a test set and the regression method OLS, ridge and lasso model using ''optimal'' \(\lambda \)-parameters. The test set here is for the real Franke function, but it correspond to the part of the data set omitted in the training data.\relax }{figure.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces The lowest MSE and highest R\(^2\) estimate with corresponding polynomail degress and lambda value for figure \ref  {fig:optimal_r2_data}, \ref  {fig:optimal_r2_franke}, \ref  {fig:optimal_mse_data} and \ref  {fig:optimal_mse_franke}. The \(\lambda \)-paramters used for ridge is \(\lambda = 0.002458\) and \(\lambda = 0.03091\) and for lasso \(\lambda = 7.2605\cdot 10^{-6}\) and \(\lambda = 3.7727\cdot 10^{-5}\) for Franke and data respectively.\relax }}{13}{table.caption.39}\protected@file@percent }
\newlabel{tab:06}{{VI}{13}{The lowest MSE and highest R\(^2\) estimate with corresponding polynomail degress and lambda value for figure \ref {fig:optimal_r2_data}, \ref {fig:optimal_r2_franke}, \ref {fig:optimal_mse_data} and \ref {fig:optimal_mse_franke}. The \(\lambda \)-paramters used for ridge is \(\lambda = 0.002458\) and \(\lambda = 0.03091\) and for lasso \(\lambda = 7.2605\cdot 10^{-6}\) and \(\lambda = 3.7727\cdot 10^{-5}\) for Franke and data respectively.\relax }{table.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces The MSE and R\(^2\) scores for the entire data set with the models with the best scores from the test data in table \ref  {tab:06} for OLS, ridge and lasso.\relax }}{13}{table.caption.40}\protected@file@percent }
\newlabel{tab:07}{{VII}{13}{The MSE and R\(^2\) scores for the entire data set with the models with the best scores from the test data in table \ref {tab:06} for OLS, ridge and lasso.\relax }{table.caption.40}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces The MSE and R\(^2\) scores for the entire data set with the models with the best scores from the test data in table \ref  {tab:07} for OLS, ridge and lasso. The models are now created based on the entire data set not only the training set.\relax }}{13}{table.caption.41}\protected@file@percent }
\newlabel{tab:07_2}{{VIII}{13}{The MSE and R\(^2\) scores for the entire data set with the models with the best scores from the test data in table \ref {tab:07} for OLS, ridge and lasso. The models are now created based on the entire data set not only the training set.\relax }{table.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces A 3D plot of the 13\(^\text  {th}\) order Ridge model with \(\lambda = 0.03091\) and the Franke function.\relax }}{13}{figure.caption.42}\protected@file@percent }
\newlabel{fig:ridge_3d}{{12}{13}{A 3D plot of the 13\(^\text {th}\) order Ridge model with \(\lambda = 0.03091\) and the Franke function.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The MSE between a OLS model for the Franke data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order. This is done \(N=50\) times with random folds and the plot shows the average.\relax }}{13}{figure.caption.43}\protected@file@percent }
\newlabel{fig:test_vs_trainlarge_n}{{13}{13}{The MSE between a OLS model for the Franke data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order. This is done \(N=50\) times with random folds and the plot shows the average.\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Terrain data}{14}{section*.44}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Colormesh plot of the terrain data used from file ``SRTM\_data\_Norway\_2" after being resampled to a size of \(400\times 200\).\relax }}{14}{figure.caption.45}\protected@file@percent }
\newlabel{fig:terrain_data}{{14}{14}{Colormesh plot of the terrain data used from file ``SRTM\_data\_Norway\_2" after being resampled to a size of \(400\times 200\).\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The MSE between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the MSE plotted is the mean of the calculated MSE i k-fold.\relax }}{14}{figure.caption.46}\protected@file@percent }
\newlabel{fig:test_vs_train2}{{15}{14}{The MSE between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the MSE plotted is the mean of the calculated MSE i k-fold.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }}{14}{figure.caption.47}\protected@file@percent }
\newlabel{fig:test_vs_train3}{{16}{14}{The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }{figure.caption.47}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IX}{\ignorespaces The error estimates MSE and R\(^2\) from figure \ref  {fig:test_vs_train2} and \ref  {fig:test_vs_train3} for 15\(^\text  {th}\) degree polynomial.\relax }}{15}{table.caption.48}\protected@file@percent }
\newlabel{tab:08}{{IX}{15}{The error estimates MSE and R\(^2\) from figure \ref {fig:test_vs_train2} and \ref {fig:test_vs_train3} for 15\(^\text {th}\) degree polynomial.\relax }{table.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }}{15}{figure.caption.49}\protected@file@percent }
\newlabel{fig:ridge_bad}{{17}{15}{The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }}{16}{figure.caption.50}\protected@file@percent }
\newlabel{fig:lasso_bad}{{18}{16}{The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The \(\lambda \)-values which minimizes the test error in k-fold cross validation with 4-folds using ridge regression. The plotted \(\lambda \)-values are the mean of N = 5 k-fold runs with randomized folds pr complexity.\relax }}{16}{figure.caption.51}\protected@file@percent }
\newlabel{fig:lambda_pr_degree_terrain}{{19}{16}{The \(\lambda \)-values which minimizes the test error in k-fold cross validation with 4-folds using ridge regression. The plotted \(\lambda \)-values are the mean of N = 5 k-fold runs with randomized folds pr complexity.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces The MSE between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the MSE plotted is the mean of the calculated MSE i k-fold.\relax }}{16}{figure.caption.52}\protected@file@percent }
\newlabel{fig:test_vs_train2ridge}{{20}{16}{The MSE between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the MSE plotted is the mean of the calculated MSE i k-fold.\relax }{figure.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {X}{\ignorespaces The error estimates MSE and R\(^2\) from figure \ref  {fig:test_vs_train2} and \ref  {fig:test_vs_train3} for 15\(^\text  {th}\) degree polynomial.\relax }}{17}{table.caption.53}\protected@file@percent }
\newlabel{tab:09}{{X}{17}{The error estimates MSE and R\(^2\) from figure \ref {fig:test_vs_train2} and \ref {fig:test_vs_train3} for 15\(^\text {th}\) degree polynomial.\relax }{table.caption.53}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XI}{\ignorespaces The error estimates MSE and R\(^2\) from figure \ref  {fig:test_vs_train2} and \ref  {fig:test_vs_train3} for 15\(^\text  {th}\) degree polynomial.\relax }}{17}{table.caption.54}\protected@file@percent }
\newlabel{tab:10}{{XI}{17}{The error estimates MSE and R\(^2\) from figure \ref {fig:test_vs_train2} and \ref {fig:test_vs_train3} for 15\(^\text {th}\) degree polynomial.\relax }{table.caption.54}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XII}{\ignorespaces The error estimates MSE and R\(^2\) from figure \ref  {fig:test_vs_train2} and \ref  {fig:test_vs_train3} for 15\(^\text  {th}\) degree polynomial.\relax }}{17}{table.caption.55}\protected@file@percent }
\newlabel{tab:11}{{XII}{17}{The error estimates MSE and R\(^2\) from figure \ref {fig:test_vs_train2} and \ref {fig:test_vs_train3} for 15\(^\text {th}\) degree polynomial.\relax }{table.caption.55}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XIII}{\ignorespaces The error estimates MSE and R\(^2\) from figure \ref  {fig:test_vs_train2} and \ref  {fig:test_vs_train3} for 15\(^\text  {th}\) degree polynomial.\relax }}{17}{table.caption.56}\protected@file@percent }
\newlabel{tab:12}{{XIII}{17}{The error estimates MSE and R\(^2\) from figure \ref {fig:test_vs_train2} and \ref {fig:test_vs_train3} for 15\(^\text {th}\) degree polynomial.\relax }{table.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }}{17}{figure.caption.57}\protected@file@percent }
\newlabel{fig:hist_terrain_ols}{{21}{17}{The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }}{18}{figure.caption.58}\protected@file@percent }
\newlabel{fig:colormap_terrain_model}{{22}{18}{The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }}{18}{figure.caption.59}\protected@file@percent }
\newlabel{fig:colormap_terrain_model5th}{{23}{18}{The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }{figure.caption.59}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{18}{section*.60}\protected@file@percent }
\newlabel{sec:Discussion}{{V}{18}{}{section*.60}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Data sets}{19}{section*.61}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{20}{section*.62}\protected@file@percent }
\newlabel{sec:Conclusion}{{VI}{20}{}{section*.62}{}}
\@writefile{toc}{\appendix }
\@writefile{toc}{\contentsline {section}{\numberline {A}Tables}{20}{section*.63}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {XIV}{\ignorespaces The \(\beta \)-values for the OLS, Ridge and Lasso regression methods with their respective confidence interval. For Ridge \(\lambda = 4.95\cdot 10^{-3}\) while for Lasso \(\lambda = 3.66\cdot 10^{-5}\).\relax }}{20}{table.caption.64}\protected@file@percent }
\newlabel{tab:13}{{XIV}{20}{The \(\beta \)-values for the OLS, Ridge and Lasso regression methods with their respective confidence interval. For Ridge \(\lambda = 4.95\cdot 10^{-3}\) while for Lasso \(\lambda = 3.66\cdot 10^{-5}\).\relax }{table.caption.64}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XV}{\ignorespaces The \(\beta \)-values for the OLS regression method using 10 folds in k-fold cross validation on the training data in a train/test data set. The confidence interval is 95\%, and is calculated by taking the standard deviation for all the calculated \(\beta _i\)-coefficients i.e the std for \(\beta _0\), \(\beta _1\), \(\beta _2\) etc.\relax }}{21}{table.caption.65}\protected@file@percent }
\newlabel{tab:14}{{XV}{21}{The \(\beta \)-values for the OLS regression method using 10 folds in k-fold cross validation on the training data in a train/test data set. The confidence interval is 95\%, and is calculated by taking the standard deviation for all the calculated \(\beta _i\)-coefficients i.e the std for \(\beta _0\), \(\beta _1\), \(\beta _2\) etc.\relax }{table.caption.65}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Figures}{22}{section*.66}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda \) values. The \(\lambda \) which gives the minimum MSE for a 5\(^\text  {th}\) order polynomial is highlighted.\relax }}{22}{figure.caption.67}\protected@file@percent }
\newlabel{fig:Vary_lambda_R2}{{24}{22}{The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda \) values. The \(\lambda \) which gives the minimum MSE for a 5\(^\text {th}\) order polynomial is highlighted.\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces The MSE between the test set and the predicted model using Lasso regression for different polynomial orders and \(\lambda \) values. The \(\lambda \) which gives the minimum MSE for a 5\(^\text  {th}\) order polynomial is highlighted. The model is calculated using 2000 iterations pr \(\lambda \)-value.\relax }}{22}{figure.caption.68}\protected@file@percent }
\newlabel{fig:Vary_lambda_L2}{{25}{22}{The MSE between the test set and the predicted model using Lasso regression for different polynomial orders and \(\lambda \) values. The \(\lambda \) which gives the minimum MSE for a 5\(^\text {th}\) order polynomial is highlighted. The model is calculated using 2000 iterations pr \(\lambda \)-value.\relax }{figure.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces The R\(^2\) between a test set from the data set and the regression method OLS, ridge and lasso model using ''optimal'' \(\lambda \)-parameters.\relax }}{22}{figure.caption.69}\protected@file@percent }
\newlabel{fig:optimal_mse_franke}{{26}{22}{The R\(^2\) between a test set from the data set and the regression method OLS, ridge and lasso model using ''optimal'' \(\lambda \)-parameters.\relax }{figure.caption.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces The R\(^2\) between a test set and the regression method OLS, ridge and lasso model using ''optimal'' \(\lambda \)-parameters. The test set here is for the real Franke function, but it correspond to the part of the data set omitted in the training data.\relax }}{22}{figure.caption.70}\protected@file@percent }
\newlabel{fig:optimal_mse_data}{{27}{22}{The R\(^2\) between a test set and the regression method OLS, ridge and lasso model using ''optimal'' \(\lambda \)-parameters. The test set here is for the real Franke function, but it correspond to the part of the data set omitted in the training data.\relax }{figure.caption.70}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces The ideal \(\lambda \) values for the Franke function.\relax }}{23}{figure.caption.71}\protected@file@percent }
\newlabel{fig:optimal_lambdas}{{28}{23}{The ideal \(\lambda \) values for the Franke function.\relax }{figure.caption.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces A 3D plot of the 4\(^\text  {th}\) order OLS model from table \ref  {tab:08} beside the Franke function.\relax }}{23}{figure.caption.72}\protected@file@percent }
\newlabel{fig:ridge_3d}{{29}{23}{A 3D plot of the 4\(^\text {th}\) order OLS model from table \ref {tab:08} beside the Franke function.\relax }{figure.caption.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces A 3D plot of the 15\(^\text  {th}\) order Lasso model from table \ref  {tab:08} with \(\lambda = 3.7727\cdot 10^{-5}\) beside the Franke function.\relax }}{23}{figure.caption.73}\protected@file@percent }
\newlabel{fig:ridge_3d}{{30}{23}{A 3D plot of the 15\(^\text {th}\) order Lasso model from table \ref {tab:08} with \(\lambda = 3.7727\cdot 10^{-5}\) beside the Franke function.\relax }{figure.caption.73}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {a}Terrain}{23}{section*.74}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }}{23}{figure.caption.75}\protected@file@percent }
\newlabel{fig:hist_terrain_ridge}{{31}{23}{The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }{figure.caption.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }}{24}{figure.caption.76}\protected@file@percent }
\newlabel{fig:hist_terrain_lasso}{{32}{24}{The R\(^2\) between a OLS model for the terrain data set and both the training and test data separately in k-fold cross validation using the entire data sat as a basis for the exclusion of folds. In this case there are used 5 folds pr polynomial order, the R\(^2\) plotted is the mean of the calculated R\(^2\) in k-fold.\relax }{figure.caption.76}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Proofs}{24}{section*.77}\protected@file@percent }
\newlabel{sec:proof}{{C}{24}{}{section*.77}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {a}Bias-variance tradeoff proof}{24}{section*.78}\protected@file@percent }
\bibdata{project_1Notes,references}
\bibcite{MHJ_LinReg}{{1}{}{{}}{{}}}
\bibcite{wiki:Regression_analysis}{{2}{}{{}}{{}}}
\bibstyle{plain}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {}Referanser}{25}{section*.79}\protected@file@percent }
\newlabel{LastBibItem}{{2}{25}{}{section*.79}{}}
\newlabel{LastPage}{{}{25}{}{}{}}
