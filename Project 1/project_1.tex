\documentclass[uio,jmp,amsmath,amssymb,reprint,nofootinbib]{revtex4-1}

\usepackage[utf8]{inputenc}
\usepackage[norsk]{babel}
\usepackage{amsmath,graphicx,varioref,verbatim,amsfonts,geometry,grffile}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[colorlinks]{hyperref}
\usepackage{flafter}
\usepackage{float}
\usepackage{placeins}
\usepackage{fancyvrb}
\usepackage{comment}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{subcaption}

% Document formatting
\setlength{\parindent}{0mm}
\setlength{\parskip}{1.5mm}
%Color scheme for listings
\usepackage{textcomp}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
%Listings configuration
\usepackage{listings}
%Hvis du bruker noe annet enn python, endre det her for å få riktig highlighting.
\lstset{
	backgroundcolor=\color{lbcolor},
	tabsize=4,
	rulecolor=,
	language=python,
        basicstyle=\scriptsize,
        upquote=true,
        aboveskip={1.5\baselineskip},
        columns=fixed,
	numbers=left,
        showstringspaces=false,
        extendedchars=true,
        breaklines=true,
        prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
        frame=single,
        showtabs=false,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941}
        }
        
\newcounter{subproject}
\renewcommand{\thesubproject}{\alph{subproject}}
\newenvironment{subproj}{
\begin{description}
\item[\refstepcounter{subproject}(\thesubproject)]
}{\end{description}}

\numberwithin{equation}{section}
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math

\newcommand{\e}{\mathrm{e}}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lsb}{\left[}
\newcommand{\rsb}{\right]}
\newcommand{\infint}{\int_{-\infty}^\infty}
\newcommand{\pdot}{\boldsymbol{\cdot}}

\lstset{inputpath=C:/Users/Jon Andre Ottesen/Documents/UiO/H2019/Python/FYS-STK4155/Project1}
\graphicspath{{C:/Users/Jon Andre Ottesen/Documents/UiO/H2019/Python/FYS-STK4155/Project1/Results/}{C:/Users/Jon Andre Ottesen/Documents/UiO/H2019/Python/FYS-STK4155/Project1/Results_terrain/}}



\begin{document}

\title{FYS-STK4155 – Applied data analysis and machine learning\\ Project 1 - Regression analysis and resampling methods}% Force line breaks with \\

\author{Jon A Ottesen}
%{Department of physics, University of Oslo}%Lines break automatically or can be 
\date{\today}

\begin{abstract}
The main topic of this projects is to study different regression methods including Ordinary Least Squares, Ridge regression and Lasso regression. These methods in conjunction with resampling techniques such as k-fold cross-validation are used in polynomial fitting on the 2-dimensional Franke function surface with added Gaussian noise and terrain data. The different methods give varied result in reproducing the original surface on a test section with Ridge giving the smallest \(MSE = 0.0075\) with respect to the Franke function when predicting data, whereas lasso gave the worst estimate with \(MSE = 0.0087\). Finally real-life data is the analyzed using the same techniques as the Franke function with OLS giving the best error estimates with \(R^2 = 0.8657\) for 15\(^\text{th}\) degree polynomial over ridge, with lasso preforming worst.

\end{abstract}

\maketitle


\section{Introduction}\label{sec:Introduction}

The first paper on regression methods was published by Legendre in 1805 and Gauss in 1809 about the least square method\cite{wiki:Regression_analysis}. Albeit being a relatively simple theory developed before the computational era of statistics its importance cannot be overstated, and is still prominent and even outperforming newer more complex methods in some cases. The simple nature of the least square theory makes it an excellent starting point for further studies in regression theory while still giving decent results in real-life data-analysis. Especially when applied together with resampling methods which has a vital role in modern statistical data analysis.

Regression methods are often used in conjunction with measured data to determine the underlying patterns. Unlike in most research where linear regression is used as a tool for data-analysis, it will here be the main object of study itself. To stimulate a more real-life usage of linear regression actual data\footnote{\url{https://github.com/CompPhysics/MachineLearning/tree/master/doc/Projects/2019/Project1/DataFiles}} is used together with generated data from  Franke's function. The central themes are thus resampling methods, error analysis such as the mean squared error, the bias-variance tradeoff and most importantly the main linear regression methods themselves: OLS, rigde and lasso regression.

%Using different linear regression methods and error analysis such as the mean squared error(from now called MSE)

%The aim of this project is to study the application of different linear regression methods in conjunction with resampling methods. Error analysis such as the mean squared error (from now called MSE) is later applied to study the bias-variance tradeoff.

\section{Theory}\label{sec:Theory}

As a basis for most of the theory below we will assume the data follows this form:
\begin{equation}\label{eq:01}
    \bm{y} = f(\bm{x}) + \bm{\epsilon}
\end{equation}
where \(\bm{y}\) is the data-sample, f is the function describing the data and \(\bm{\epsilon}\) is the noise of the data following a Gaussian distribution with a mean of 0.

\subsection{Moments and error analysis}

\subsubsection{Moments in statistics}

To describe, analyse and understand data, statistical knowledge is essential. Various moments in statistics are therefore used when describing key elements of data, models or functions. In our case we want to describe and understand the model created with linear regression, and for this the 1\(^\text{st}\) and 2\(^\text{and}\) order moments are a necessity.

For a real valued continuous function \(g(x)\) for real-valued x, the n-th moment is given by
\begin{equation}
    \mu_n(c) = \infint (x-c)^ng(x)dx
\end{equation}
with c as a real variable. By restricting g as a normalized non-negative function the 1\(^\text{st}\) moment around \(c=0\) is the mean value given by
\begin{align}
    \mu = \infint xg(x)dx.
\end{align}
Higher order moments are often defined around the mean:
\begin{align}
    \mu_n(\mu) = \infint (x-\mu)g(x)dx
\end{align}
to provide more qualitative information about the distribution. The 2\(^\text{nd}\) moment is the variance
\begin{equation}
    \mu_2(\mu) = \sigma^2 = \infint (x-\mu)^2g(x)dx
\end{equation}
where \(\sigma\) is the standard-deviation.

So far I have only looked at the continuous case where the probability distribution is know. In the non continuous case with a sample of finite size the true moments of the distribution can realistically only be approximated. There is however theoretically possible to get the correct values by making use of the central limit theorem.

For a finite sample of size n taken from the function g the 1\(^\text{st}\) moment i.e the mean value is for the sample is given by:
\begin{equation}\label{eq:02}
    \overline{g^s} = \frac{1}{n}\sum_{i=1}^n g_i^s
\end{equation}
The 2\(^\text{an}\) moment i.e variance is for the sample
\begin{align}\label{eq:03}
    \text{Var}(g^s) = \frac{1}{n}\sum_{i=1}^n (g^s_i - \overline{g^s})^2
\end{align}
where in both cases \(g^s\) is a sample of data from the distribution \(g(x)\). Note however that \(\overline{g^s}\) and \(\text{Var}(g^s)\) are only approximations to the real mean \(\mu\) and variance \(\mu_2\) for the distribution g.

Let's assume we are finite sample of size n \(\bm{y}\) which obeys function \ref{eq:01}, where the function \(\bm{\hat{f}}\) is the approximation of \(f(\bm{x})\) by regression methods. The variance of the noise \(\bm{\epsilon}\) can be approximated by
\begin{align}
    \text{Var}(\epsilon) &\approx \frac{1}{n - m}\sum_{i=1}^n (y_i - \hat{g_i})^2
\end{align}
where \(m = p-1\) and p is the number of columns in the design matrix \(\bm{X}\)(\cite{HastieTrevor2009TEoS} page 47).

\subsubsection{Error analysis}

There are a multitude of different cost functions for evaluating the error in a model. Among them are the mean square error(MSE) and the \(R^2\) score function which are respectivly given by
\begin{align}\label{eq:05}
    MSE(\bm{y}, \bm{\tilde{y}}) &= \frac{1}{n}\sum_{i=1}^n(y_i - \tilde{y_i})^2\\ \label{eq:11}
    R^2(\bm{y}, \bm{\tilde{y}}) &= 1 - \frac{\sum_{i=1}^n(y_i - \tilde{y_i})^2}{\sum_{i=1}^n(y_i - \overline{y})^2}. 
\end{align}
Notice how for \(m=0\) in equation \ref{eq:03} the we are left with the equation for the MSE.

\subsection{Linear regression methods}

The very basis of linear regression is based around the assumption that the form of the data can be written on the form of equation \ref{eq:01}. This form can again be written as
\begin{equation}
    \bm{y} = \bm{X}\bm{\beta} + \bm{\epsilon}
\end{equation}
where \(\bm{y}, \bm{\epsilon} \in \mathbb{R}^{n\times 1}\) whereas \(\bm{\beta} \in \mathbb{R}^{p\times 1}\) and \(\bm{X}  \in \mathbb{R}^{n\times p}\) is the design matrix.

The goal of linear regression is thus to approximate \(\bm{y}\) with
\begin{align}\label{eq:10}
    \bm{\tilde{y}} = \bm{X}\bm{\beta}
\end{align}
by optimizing \(\bm{\beta}\). 

\subsubsection{Ordinary least squares}

The ordinary least square method optimizes \(\bm{\beta}\) by minimizing the MSE cost function
\begin{align}
    C(\bm{X, \beta}) &= \frac{1}{n}\sum_{i=1}^n(y_i-\tilde{y}_i)\\
    &= \frac{1}{n}\lsb \lp \bm{y} - \bm{\tilde{y}}\rp^T\lp \bm{y} - \bm{\tilde{y}}\rp\rsb\\
    &= \frac{1}{n}\lsb \lp \bm{y} - \bm{X}\bm{\beta}\rp^T\lp \bm{y} - \bm{X}\bm{\beta}\rp\rsb.
\end{align}
The steps is to take following derivative
\begin{align}
    \frac{\partial C(\bm{X, \beta})}{\partial\bm{\beta}} = 0
\end{align}
which result in the OLS formula
\begin{equation}\label{eq:06}
    \bm{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}.
\end{equation}
A more rigorous version of the same derivation is found in \cite{MHJ_LinReg}.

Another way of deriving the OLS formula is by considering
\begin{equation}
    \bm{\tilde{y}} = \text{proj}_{\text{Col X}}\bm{y}.
\end{equation}
Following this there exist a \(\bm{\beta}\) such that
\begin{equation}
    \bm{\tilde{y}} = \bm{X}\bm{\beta}.
\end{equation}
A consequence of having \(\bm{\tilde{y}} = \text{proj}_{\text{Col X}}\bm{y}\) is that \(\bm{y} - \bm{\tilde{y}}\) is orthogonal to the vector space spanned by \(\bm{X}\) as stated by the Orthogonal Decomposition Theorem. Therefore any column in \(\bm{X}\) must be orthogonal to \(\bm{y} - \bm{\tilde{y}}\) such that
\begin{align}
    \bm{X}_i\pdot (\bm{y} - \bm{\tilde{y}}) = 0.
\end{align}
This implies
\begin{equation}
    \bm{X}^T(\bm{y} - \bm{\tilde{y}}) = 0
\end{equation}
and finally
\begin{equation}
    \bm{X}^T\bm{X}\bm{\beta} = \bm{X}^T\bm{\tilde{y}} 
\end{equation}
which can be rearranged to equation \ref{eq:06}. The derivation is taken from \citep{LayDavidC2016Laai} in chapter 6.5.

Both derivations are equal in the sense that they yield the OLS formula, but their interpretations are different.

\subsubsection{Ridge and Lasso regression}

Ridge and lasso regression are two shrinkage methods in linear regression. They work by imposing a penalty on the regression coefficients based on their size. This is done by minimizing their respective cost functions, and for ridge regression this cost function is given by
\begin{align}
    C(\bm{X,\beta})_{\text{Ridge}} = \frac{1}{n}\lsb \lp \bm{y} - \bm{X\beta}\rp^T\lp \bm{y} - \bm{X\beta}\rp + \lambda \bm{\beta}^T\bm{\beta}\rsb
\end{align}
which is variant of the MSE with \(\lambda > 0\). The coefficients for \(\bm{\beta}\) in ridge regression can be written in the following closed form
\begin{align}\label{eq:07}
    \bm{\beta} = (\bm{X}^T\bm{X} + \lambda\bm{I})^{-1}\bm{X}^T\bm{y}
\end{align}
where \(\bm{I} \in \mathbb{R}^{p\times p}\). Further analysis of equation \ref{eq:07} would reveal how the \(\lambda\) parameter shrinks terms i beta, but that is not necessary for this project.

Lasso regression much like OLS and ridge regression is a minimization problem. However unlike OLS and ridge regression there exist no analytical solution to the cost function minimized in lasso regression:
\begin{equation}
C(\boldsymbol{X},\boldsymbol{\beta})=\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_1.
\end{equation}

To solve lasso regression the implementation of gradient decent types of algorithms are a necessity. Such algorithms are outside the scope of this project and will not be covered in any depth, instead preexisting tools will be utilized.

\subsubsection{Confidence intervals}

The confidence intervals for the \(\beta_i\) terms from equation \ref{eq:06} at \(68\%\) confidence intervals for OLS is given by:
\begin{align}\label{eq:12}
\sigma(\beta_i)^{\text{OLS}} = \sigma\sqrt{(\bm{X}^T\bm{X})_{ii}}.
\end{align}
Thus the variance of \(\bm{\beta}^{\text{OLS}}\) is given by the diagonal of the square matrix \(\bm{X}^T\bm{X}\) multiplied with the variance of the error in function \ref{eq:01}. For ridge regression the confidence intervals of the \(\beta_i\) terms from equation \ref{eq:07} at \(68\%\) is given by
\begin{align*}
&\sigma(\beta_i)^{\text{Ridge}} = \\
&\sigma\sqrt{ \lsb\lp\bm{X^TX} + \lambda\bm{I}\rp^{-1}\bm{X^TX}\lp\lp\bm{X^TX} + \lambda\bm{I}\rp^{-1}\rp^T\rsb_{ii} }.
\end{align*}

\subsubsection{SVD}

Numerical matrix inversion has a tendency to become unstable for larger matrices \(\bm{X}\). To avoid this problem, SVD will be utilized to provide a more stable algorithm. Without going into to much technicality the goal of this subsection is to formulate mainly equation \ref{eq:06} and \ref{eq:07} but also their confidence intervals in terms of \(\bm{U, V}\) and \(\bm{\Sigma}\).

Any \(n\times p\) matrix \(\bm{X}\) with rank r can be written as
\begin{align}
\bm{X} = \bm{U\Sigma V}^T
\end{align}
with \(\bm{U}\) being an orthogonal \({n\times n}\) matrix and \(\bm{V}\) an orthogonal \({p\times p}\) matrix. Whereas \(\bm{\Sigma}\) is a \(n\times p\) matrix where the first r diagonal elements are the singular values of \(\bm{X}\)\cite{MHJ_LinReg}. A small note is that \(\bm{V}^{-1} = \bm{V}^T\) and \(\bm{U}^{-1} = \bm{U}^T\) since they are orthogonal matrices.

The formulas for both OLS and ridge regression can be rewritten in terms of \(\bm{\Sigma, U, V}\) as
\begin{align}\label{eq:08}
\bm{\beta}_{OLS} &= \bm{V}(\bm{\Sigma})^{-1}\bm{U}^T\bm{y}\\ \label{eq:09}
\bm{\beta}_{Ridge} &= \bm{V}(\bm{\Sigma}^2 + \lambda\bm{I})^{-1}\bm{\Sigma}^T\bm{U}^T\bm{y}.
\end{align}
Both derivations can be found at \cite{SVD}. Further the confidence intervals for the \(\beta_i\) from equation \ref{eq:12} can be rewritten as
\begin{equation}\label{eq:13}
\sigma(\beta_i)^{\text{OLS}} =\sqrt{ \lp\bm{V}\lp\bm{\Sigma}^T\bm{\Sigma}\rp^{T}\bm{V}^T\rp_{ii}}\sigma
\end{equation}
while the confidence interval for Ridge regression takes the following form:
\begin{align}\label{eq:14}
&\sigma(\beta_i)^{\text{Ridge}} = \sigma \sqrt{ \lp\bm{V}\lp \bm{\Sigma^2 + \lambda}\rp^{-2}\bm{\Sigma^2}\bm{V^T}\rp_{ii}}
\end{align}
where \(\bm{\lambda} = \lambda\bm{I}\). The derivation of equation \ref{eq:13} and \ref{eq:14} is done in a similar manner to equation \ref{eq:08} and \ref{eq:09} which is derived in \cite{SVD}.


\subsection{Resampling and Bias-variance tradeoff}

\subsubsection{Reasmpling methods}

There exist a multitude of different resampling methods, but the common trope is repeatedly refitting a model by drawing(often randomly) samples from a larger data set. By repeatedly drawing out samples from a larger set the goal is to utilize the central limit theorem and limit the correct value for a large number of runs, but also compare models between different sets of the same data.

K-fold cross validation is based around the principle of dividing a data set into k equally sized(if possible) folds with \(k \leq (\text{length of the data})\). Than k-1 of the folds are used as training data in linear regression while one fold is used as test data. Thereafter the prediction error and mean among other things are evaluated. This is redone k times but with a different fold for the test data each time such that all k-folds are used as test data. By the central limit theorem the evaluated values are than decent approximations to the real value. Another important aspect would be to see how the results differ. Albeit not essential the data set should be shuffled before diving into folds to avoid an unbalanced representation of the data set.


\subsubsection{Bias-variance tradeoff}

When creating a model an important application is the ability to predict using the model. The term overfitting is often used when describing the action of fitting a model to close to the data such that the model no longer describes the underling function \(f(\bm{x})\). The bias-variance tradeoff is closely related to overfitting and underfitting data. In such a way that our model must be complex enough to have low bias error while avoiding large variance errors.

The MSE from equation \ref{eq:05} can be rewritten as
\begin{align}\label{eq:bias}
\begin{split}
&MSE(\bm{y}, \bm{\tilde{y}}) \\
&= \frac{1}{n}\sum_i(f_i-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right])^2+\frac{1}{n}\sum_i(\tilde{y}_i-\mathbb{E}\left[\boldsymbol{\tilde{y}}\right])^2+\sigma^2.
\end{split}
\end{align}
In equation \ref{eq:bias} the first term is the bias squared term whereas the second is the variance term, and the last is the variance of the inevitable error from equation \ref{eq:01}. The derivation is given in section \ref{sec:proof} in the appendix.

The bias error is the error which reveals the difference between the model and the actual function. For large bias errors the model has a large deviation from underlying function f. High bias can cause the model to miss important relations in the data. The bias error is large in underfitting.

The variance error is the error that reveals how sensitive the model is to the noise. For high variances the model is not only fitted on the function f, but also on the error in the data set. If the noise were changed the model would change drastically. This error is most important when overfitting.

The main goal of a model is to find the ideal combination of bias and variance error such that the error is minimized, while also being able to predict important features of the underlying function f.

\section{Method}\label{sec:Method}

All code can be found in my github at\\
\url{https://github.com/JonOttesen/FYS-STK4155/tree/master/Project1}\\
and \texttt{regression.py} is the general module for the regression analysis, \texttt{results.py} is the program producing the results for the Franke function and \texttt{terrain.py} is the file which produces the results for the terrain data. The renaming python files \texttt{latex\_print.py} is used for printing latex tables wheras \texttt{testing.py} is a testing file and is not used in producing results.

\subsection{Preparations}

There are two sets of data used during this project. The first is self-generated through the 2D Franke function with added noise normally distributed, while the other set is terrain data found at\\
\url{https://github.com/CompPhysics/MachineLearning/tree/master/doc/Projects/2019/Project1/DataFiles}\\
and the datafile used is the \texttt{SRTM\_data\_Norway\_2.tif} file.

The self-generated data is generated with the Franke function on a \(n\times n\)\footnote{It's not requirement for grid to be \(n\times n\), however making the grid \(n\times m\) with \(m\neq n\) would not have any noticable effect on the results for a reasonable sized n and m.} meshgrid with \(x,y\in [0,1]\) selected by random from a uniform distribution, and each point has a added normally distributed noise with \(\mu = 0\) and \(\sigma = 1\) in the z-direction. Thus the function used in the data-generation is
\begin{equation}
f_{F}(x,y) = F(x, y) + \mathcal{N}(0,1)
\end{equation}
with \(F(x,y)\) being the Franke function. Note that I used the \textit{seed 42} when generating all the random numbers, this was to simulate a real-life data set where the measured data is static.

In the terrain data I was given all the data points, thus I only had to generate the grid. Before doing that I cut away the last row and column such that my data set was of \(3600\times 1800\) data points. I further decrease the size of the data to \(400\times 200\) by taking the average value of \(9\times 9\) non overlapping grids inside the data set.

With a reduced data set I created the x and y grid by creating a grid of 400 linearly spaced x-values and 200 linearly spaced y-values with \(x, y \in [0, 1]\).

The last preparatory step before the analysis is to create a design matrix from the x,y datapoints in the meshgrid. This is done by flattening the meshgrids using \texttt{np.ravel} and the design matrix is created  by polynomial combinations of x and y up to a given degree n. A code snippet of the creation of X is shown here:
\begin{verbatim}
for i in range(1, k + 1):
    q = int((i)*(i + 1)/2)
    for k in range(i + 1):
        X[:,q+k] = x**(i-k) * y**k
\end{verbatim}
For a showing of the order of the different polynomial print \texttt{polynomial\_str} parameter from the \texttt{regression} class.

For all design matrices the SVD is calculated by scipy and \texttt{full\_matrices = False}.

With the SVD calculated the regression method OLS is calculated by equation \ref{eq:08} while ridge is calculated by equation \ref{eq:09}. Lasso is calculated by sklearn by \texttt{sklearn.linear\_model.Lasso}.


\subsection{Regression analysis}

For future reference when talking about the ideal \(\lambda\)-value I am referring to the \(\lambda\) which minimized the MSE and maximized the R\(^2\) error in k-fold cross validation for the excluded fold. This is done by calculating the error estimates for the excluded fold for m-different \(\lambda\)-values which remain constant throughout the k-fold. Than the \(\lambda\) which minimized the mean error in the excluded folds is the 'ideal' \(\lambda\). In table \ref{tab:01} I have tried to illustrate how the error estimate may differ for the same fold depending on the \(\lambda\)-value. In table \ref{tab:01} the ideal \(\lambda\) would be number 4 since mean is the smallest. 

\begin{table}
\begin{tabular}{|c|c|c|c|}\hline
\(\lambda\)-values & Fold 1 & Fold 2 & Fold 3 \\ \hline
\(\lambda_{1}\) & 1 & 2 & 2\\ \hline
\(\lambda_{2}\) & 2 & 2 & 2 \\ \hline
\(\lambda_{3}\) & 1 & 2 & 1 \\ \hline
\(\lambda_{4}\) & 1 & 1.5 & 1 \\ \hline
\end{tabular}
\caption{A illustration of how the error estimate can differ depending on the fold and the \(\lambda\) value. The ideal \(\lambda\) would here be \(\lambda_4\).}
\label{tab:01}
\end{table}

\subsubsection{Franke function}

Most of the actual data analysis is pretty similar between the different data sets. 
In the Franke function data set the first step was to calculate the OLS, ridge and lasso models for the entire design matrix with 5\(^{th}\) degree polynomial complexity. The resulting models was than used to calculate the MSE and R\(^2\) error by equation \ref{eq:05} and \ref{eq:11} respectively. The error estimates was calculated both with respect to the data set and to the real Franke function without noise. How the \(\lambda\) values used for ridge lasso was found is explained in the next couple of paragraphs. The confidence intervals for \(\beta\) was also calculated by equation \ref{eq:13} for OLS and equation \ref{eq:14} for ridge regression. In lack of a proper way to calculate the confidence interval for lasso, equation \ref{eq:14} was used.

The next step was to split the data sets into training and test data using the \texttt{test\_train\_split} function from sklear. The ratio for the split used was \(70\%\) training data and \(30\%\) test data. The training data was than used to fit linear regression models for OLS, ridge and lasso with the MSE and R\(^2\) errors calculated using the test data and the test section of the Franke function without noise. Further for ridge and lasso regression the MSE and R\(^2\) error estimates was calculated for a multitude of different \(\lambda\)-values, and the \(\lambda\) giving the minimum was found. This \(\lambda\) is the same used when fitting the entire data set in the previous paragraph. The confidence intervals for \(\beta\) in OLS was also calculated by equation \ref{eq:13}, the same was done for ridge and lasso, but with equation \ref{eq:14} and their respective minimum \(\lambda\)-values.

Now the resampling technique k-fold cross validation was implemented. The \(\beta\)-coefficients, MSE and R\(^2\) values were stored for each exclusion of a fold. The error quantities were calculated both with respect to the test data and training data, and stored. Finally the mean and variance for each measured quantity was calculated. This was done for \(k=10\) folds.

The next step was to use k-fold cross validation with 5-folds to calculate the MSE and R\(^2\) for test and train data. This was done for different polynomial degrees from 0 to 15. This was repeated 50 times for each degree but with random folds and the mean of the 50 runs for each polynomial degree was plotted. The result is thus a plot of the training error and test error pr polynomial degree. This was also done for a larger data set with \(400\times 200\) data points.

The next step is to create a plot of the bias and variance. This is done by recreating the data set 100 times pr polynomial degree and creating a model each time for the training data. With 100 different models the variance in the bias-variance tradeoff was calculated by taking the variance between the \(\bm{\tilde{z}}\) in the test data. The bias was than calculated by taking the mean squared difference between the Franke function and the mean of all the \(\bm{\tilde{z}}\)-models for the test set as shown i equation \ref{eq:bias}.

In the last part the ideal \(\lambda\)-value pr complexity is used to create models for the training data in the splitted data set. This is done for polynomials from 3 to 15 degree complexity for OLS, ridge and lasso regression. The error estimates is than calculated between the model and the test data, but also between the model and the test section of the real Franke function. These models are also used to calculate the error estimates for the entire data set and finally the entire data set is used in the creation of these models with the ideal polynomial degree and \(\lambda\).

The last part which was done was to plot the models surface in 3D for both a OLS model and ridge model with the ideal \(\lambda\).

\subsubsection{Terrain data}

Unlike for the Franke data I will begin by calculating the error estimates using k-fold cross validation with 5 folds. This is done for all polynomial orders from 1 to 20 and repeated \(N=10\) times for each polynomial with random folds. In the k-fold the error estimates for both the test and training folds are calculated and in the end both are plotted side by side. The plotted values are the mean of N times.

After k-fold cross validation the data set is splitted into test and training data. The training data is used to create multiple models for ridge and lasso for varying \(\lambda\) values. For each model with different \(\lambda\) error estimates are calculated on test data. The error estimates in test data are than plotted as a function   \(\lambda\). This is done for 14, 15 and 16\(^\text{th}\) degree complexity.

In the next step the evolution of the ideal \(\lambda\)-values are studied. This is done by calculating the ideal \(\lambda\) by k-fold a total of 50 times with randomized folds, than the mean \(\lambda\)-value is calculated with corresponding standard error at \(95\%\) confidence. This is repeated for all polynomial degrees between 1\(^\text{st}\) and 15\(^\text{th}\) complexity. Finally the mean \(\lambda\)-values for each complexity is plotted with the error estimates, only the non-zero \(\lambda\) are plotted.

With the mean ideal \(\lambda\)-values calculated, k-fold cross validation is again used to calculate the error estimates for test and training. Ridge regression with the the mean ideal \(\lambda\)-values are than used to create models for all polynomial orders from 1 to 20. The ridge model with the mean ideal \(\lambda\)-value for each complexity is than used to calculate the error estimates for test and training sets. The error estimates from the test and training are than plotted in the same plot. For the polynomial degrees where there were no mean ideal \(\lambda\)-value I used \(\lambda = 0\). As earlier the k-fold was done \(N=10\) times with random folds and the average error estimates for test and training was plotted.

In the next part k-fold cross validation was again used but now with \(k=10\) folds. The error estimates for the OLS, ridge and lasso models for a 15\(^\text{th}\) degree complexity was than plotted in a histogram. The inevitable error from equation \ref{eq:01} was also calculated, this by equation \ref{eq:03}.

The final part was simply creating a model for both 15\(^\text{th}\) and 5\(^\text{th}\) degree polynomial complexity for the entire data set. Calculating the error estimates and finally plotting colormesh plots for both modeled surfaces. Ridge and lasso regression was not used in the final part.


\section{Results}\label{sec:Results}

\subsubsection{Franke function}

For most of the analysis, the data set will consist of \(81\times 81\) data points except at the end of this subsection when stated otherwise. A plot of the data set of the Franke function with added noise is shown in figure \ref{fig:Franke_w_noise}. This is the data that will be used for further analysis.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Frankewnoise}
    \caption{The Franke function with added Gaussin noise with \(\mu=0\) and \(\sigma=1\) for a grid of \(81\times 81\) randomly distributed data points taken from a uniform distribution between \([0,1]\).}
    \label{fig:Franke_w_noise}
\end{figure}

The model created by OLS, Ridge and Lasso with a complexity of 5\(^\text{th}\) degree gave the following \(\beta\)-coefficients in table \ref{tab:02} for the full data set. Note however my choice for \(\lambda\) for both Ridge and Lasso, this choice will be apparent later. The confidence intervals for OLS is caluclated by \ref{eq:13} while ridge and lasso is calculated by equation \ref{eq:14}, and all \(\sigma\) are multiplied with 1.96 to get the confidence at \(95\%\).

\begin{table}
\begin{tabular}{|c|c|c|c|}\hline
\(\beta\) & OLS & Ridge & Lasso \\ \hline
\(\beta_{0}\) & 0.375 \(\pm\) 0.401 & 0.623 \(\pm\) 0.241 & 1.1 \(\pm\) 0.379 \\ \hline
\(\beta_{1}\) & 8.04 \(\pm\) 4.6 & 4.22 \(\pm\) 1.7 & -0.211 \(\pm\) 4.23 \\ \hline
\(\beta_{2}\) & 4.28 \(\pm\) 4.52 & 3.23 \(\pm\) 1.65 & 0.537 \(\pm\) 4.15 \\ \hline
\(\beta_{3}\) & -32.0 \(\pm\) 22.5 & -15.6 \(\pm\) 4.74 & -2.43 \(\pm\) 20.3 \\ \hline
\(\beta_{4}\) & -10.3 \(\pm\) 17.1 & -1.22 \(\pm\) 4.17 & 1.27 \(\pm\) 15.9 \\ \hline
\(\beta_{5}\) & -13.9 \(\pm\) 22.9 & -12.4 \(\pm\) 4.74 & -4.18 \(\pm\) 20.6 \\ \hline
\(\beta_{6}\) & 35.9 \(\pm\) 51.3 & 9.72 \(\pm\) 6.16 & 1.3 \(\pm\) 46.3 \\ \hline
\(\beta_{7}\) & 40.2 \(\pm\) 37.4 & 10.1 \(\pm\) 6.84 & 3.14 \(\pm\) 34.8 \\ \hline
\(\beta_{8}\) & 4.59 \(\pm\) 36.3 & -4.09 \(\pm\) 6.85 & -2.39 \(\pm\) 34.0 \\ \hline
\(\beta_{9}\) & 4.93 \(\pm\) 52.8 & 7.01 \(\pm\) 6.09 & 1.44 \(\pm\) 47.4 \\ \hline
\(\beta_{10}\) & -6.6 \(\pm\) 54.3 & 9.03 \(\pm\) 6.99 & 0.325 \(\pm\) 49.2 \\ \hline
\(\beta_{11}\) & -50.2 \(\pm\) 40.7 & -7.62 \(\pm\) 9.15 & 0.0 \(\pm\) 38.3 \\ \hline
\(\beta_{12}\) & 8.07 \(\pm\) 37.2 & 9.88 \(\pm\) 9.51 & 0.0 \(\pm\) 35.1 \\ \hline
\(\beta_{13}\) & -19.1 \(\pm\) 39.4 & -5.49 \(\pm\) 9.28 & 0.0 \(\pm\) 37.3 \\ \hline
\(\beta_{14}\) & 18.2 \(\pm\) 55.9 & 10.4 \(\pm\) 6.82 & 1.71 \(\pm\) 50.4 \\ \hline
\(\beta_{15}\) & -5.77 \(\pm\) 21.5 & -8.06 \(\pm\) 4.26 & -0.0937 \(\pm\) 19.6 \\ \hline
\(\beta_{16}\) & 17.3 \(\pm\) 18.3 & -0.983 \(\pm\) 6.79 & -1.76 \(\pm\) 17.6 \\ \hline
\(\beta_{17}\) & 4.8 \(\pm\) 17.6 & -0.287 \(\pm\) 8.56 & 0.0 \(\pm\) 17.0 \\ \hline
\(\beta_{18}\) & -10.5 \(\pm\) 17.6 & -6.5 \(\pm\) 8.66 & 0.189 \(\pm\) 17.0 \\ \hline
\(\beta_{19}\) & 14.8 \(\pm\) 17.8 & 6.05 \(\pm\) 6.81 & 0.0 \(\pm\) 17.1 \\ \hline
\(\beta_{20}\) & -13.6 \(\pm\) 22.1 & -8.49 \(\pm\) 4.26 & -0.0319 \(\pm\) 20.0 \\ \hline
\end{tabular}
\caption{The \(\beta\)-values for the OLS, Ridge and Lasso regression methods with their respective confidence intervals. For Ridge \(\lambda = 4.95\cdot 10^{-3}\) while for Lasso \(\lambda = 6.34\cdot 10^{-5}\).}
\label{tab:02}
\end{table}

The linear models with the \(\beta\)-coefficients shown in table \ref{tab:02} gives the following error estimates for MSE and R\(^2\) shown in table \ref{tab:03} calculated by equation \ref{eq:05} and \ref{eq:11}. The error estimates shown are both between the model and the data set, but also between the actual Franke function and the model.

\begin{table}
\begin{tabular}{|c|c|c|c|}\hline
Error estimates & OLS & Ridge & Lasso\\ \hline
MSE Franke & 0.0053 & 0.0062 & 0.0102 \\ \hline
MSE Data & 0.998 & 0.999 & 1.01 \\ \hline
R\(^2\) Franke & 0.944 & 0.934 & 0.892 \\ \hline
R\(^2\) Data & 0.102 & 0.101 & 0.0937 \\ \hline
\end{tabular}
\caption{The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref{tab:01}. The error estimates are calculated both with respect to the actual Franke function and the data set used to create the model.}
\label{tab:03}
\end{table}

So far all the results presented are from creating models on the entire data set. From now on the data set is split at a ratio 70\% training and 30\% test data. The \(\beta\)-coefficients from the training data is given in table \ref{tab:13} in the appendix. The error estimates are given in table \ref{tab:04} and \ref{tab:05}. In table \ref{tab:04} the error estimates is calculated based on the data points used for training while the error estimates in table \ref{tab:05} is calculated based upon the test section of the data. 


\begin{table}
\begin{tabular}{|c|c|c|c|}\hline
Error estimates & OLS & Ridge & Lasso\\ \hline
MSE Franke & 0.00783 & 0.00807 & 0.011 \\ \hline
MSE Data & 1.01 & 1.01 & 1.02 \\ \hline
R\(^2\) Franke & 0.917 & 0.915 & 0.884 \\ \hline
R\(^2\) Data & 0.0994 & 0.0979 & 0.0914 \\ \hline
\end{tabular}
\caption{The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref{tab:13}. The error estimates are for the training section of the data and are calculated both with respect to the actual Franke function and the data set used to create the model.}
\label{tab:04}
\end{table}

\begin{table}
\begin{tabular}{|c|c|c|c|}\hline
Error estimates & OLS & Ridge & Lasso\\ \hline
MSE Franke & 0.00783 & 0.0079 & 0.0106 \\ \hline
MSE Data & 0.981 & 0.979 & 0.984 \\ \hline
R\(^2\) Franke & 0.917 & 0.916 & 0.888 \\ \hline
R\(^2\) Data & 0.101 & 0.103 & 0.098 \\ \hline
\end{tabular}
\caption{The error estimates MSE and R\(^2\) calculated based upon the model created by the coefficients in table \ref{tab:13}. The error estimates are for the test section of the data and are calculated both with respect to the actual Franke function and the test section of the data set used to create the model.}
\label{tab:05}
\end{table}

So far I have only blindly used two values for \(\lambda\) for both ridge and lasso regression. In figure \ref{fig:Vary_lambda_R} and \ref{fig:Vary_lambda_L} the MSE between the test set and a model for varying \(\lambda\) with ridge and lasso regression is plotted respectively. The same applies to figure \ref{fig:Vary_lambda_R2} and \ref{fig:Vary_lambda_L2}, but for a wider range of \(\lambda\)-values.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Ridge_and_smalllamba}
    \caption{The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda\) values. The \(\lambda\) which gives the minimum MSE for a 5\(^\text{th}\) order polynomial is highlighted.}
    \label{fig:Vary_lambda_R}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Lasso_and_smalllambda}
    \caption{The MSE between the test set and the predicted model using Lasso regression for different polynomial orders and \(\lambda\) values. The \(\lambda\) which gives the minimum MSE for a 5\(^\text{th}\) order polynomial is highlighted. The model is calculated using 1001 iterations pr \(\lambda\)-value.}
    \label{fig:Vary_lambda_L}
\end{figure}

Using the resampling method k-fold cross validation with 10-folds the MSE calculated between the model and the excluded fold is shown as histograms in figure \ref{fig:K_fold10_OLS}, \ref{fig:K_fold10_Ridge} and \ref{fig:K_fold10_Lasso} for OLS, ridge and lasso respectively.

For 10-folds with OLS the MSE and R\(^2\) scores from the test data in k-fold are
\begin{align*}
\text{MSE} &= 1.005\\
\text{R}^2 &= 0.0937.
\end{align*}
The standard deviation of the inevitable error from equation \ref{eq:01} is calculated for each exclusion of a fold in k-fold cross validation. The mean of all these standard deviations is
\begin{equation}
\sigma = 1.0007 \pm 0.005
\end{equation}
with 95\% confidence interval. The mean \(\beta\)-coefficients of the models from k-fold is given in table \ref{tab:14} and the error is the standard error of all the calculated \(\beta_i\)-coefficients at a 95\% confidence interval. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Hist_Olsk=10}
    \caption{The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using OLS.}
    \label{fig:K_fold10_OLS}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Hist_Ridgek=10}
    \caption{The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using Ridge with the \(\lambda\)-values given in the plot.}
    \label{fig:K_fold10_Ridge}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Hist_Lassok=10}
    \caption{The MSE between the models created and their respective excluded folds in k-fold cross validation for 10 folds using Lasso with the \(\lambda\)-values given in the plot.}
    \label{fig:K_fold10_Lasso}
\end{figure}

In figure \ref{fig:test_vs_train} the train and test errors from k-fold cross validation with \(k=5\) folds is plotted as function of polynomial complexity. The plotted values are the mean of 50 runs pr polynomial complexity but with random folds, the MSE is than the mean of the 50 runs. The same plot but for R\(^2\) is shown in figure \ref{fig:test_vs_trainR2}.

Further in figure \ref{fig:bias_varianceOLS} the bias and variance is plotted against the model complexity. The bias and variance is calculated between 100 models created by re randomizing the noise 100 times and fitting the OLS model on a train set. The bias and variance is calculated on the test section of the model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{tradeoff2MSEOLS}
    \caption{A plot of the MSE from the test and training folds in k-fold cross validation with \(k=5\) folds for the Franke data using OLS. The values are the average of \(N=50\) runs with randomized folds.}
    \label{fig:test_vs_train}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{tradeoff2R2OLS}
    \caption{A plot of the R\(^2\) score from the test and training folds in k-fold cross validation with \(k=5\) folds for the Franke data. The values are the average of \(N=50\) runs with randomized folds.}
    \label{fig:test_vs_trainR2}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{bias_varianceOLS}
    \caption{A plot of the Bias\(^2\) and Variance for the Franke data by re randomizing the noise in the data set while keeping the data points constant. The  Bias\(^2\) and Variance is calculated based on the models fitted on the test section.}
    \label{fig:bias_varianceOLS}
\end{figure}

The final part of this subsection will resolve around finding the optimal solution for solving the problem. Therefore in figure \ref{fig:optimal_r2_franke} and \ref{fig:optimal_r2_data} the R\(^2\) score for OLS, ridge and lasso is plotted against the corresponding model complexity. The model is created using a training set whereas the error is calculated using the test data. Further in figure \ref{fig:optimal_r2_franke} the error estimate is calculated with the real Franke function on the test section of the data while figure \ref{fig:optimal_r2_data} shows the error estimate for the data set. Corresponding figures for the MSE is shown in figure \ref{fig:optimal_mse_data} for the data set and \ref{fig:optimal_mse_franke} when comparing to the real Franke function. The minimum MSE and highest R\(^2\) scores is found in table \ref{tab:06} with the corresponding polynomial complexity and \(\lambda\)-parameters. For lasso and ridge regression the \(\lambda\)-parameter is my calculated ideal \(\lambda\) for the training set and is plotted in figure \ref{fig:optimal_lambdas}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{ridge_lasso_high_order_poly_data}
    \caption{The R\(^2\) between a test set from the data set and the regression methods OLS, ridge and lasso using the ''optimal'' \(\lambda\)-values for each polynomial degree and method from figure \ref{fig:optimal_lambdas}.}
    \label{fig:optimal_r2_franke}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{ridge_lasso_high_order_poly}
    \caption{The R\(^2\) score between a test set and the regression methods OLS, ridge and lasso using the ''optimal'' \(\lambda\)-values for each polynomial degree and method from figure \ref{fig:optimal_lambdas}. The test set here is for the real Franke function without noise, but it correspond to the part of the data set omitted in training the model.}
    \label{fig:optimal_r2_data}
\end{figure}

\begin{table}
\begin{tabular}{|c|c|c|c|c|}\hline
Error estimates & OLS & Ridge & Lasso & Degree\\ \hline
MSE Franke & 0.00783 & 0.0075 & 0.00868 & 5, 6, 9 \\ \hline
MSE Data & 0.979 & 0.978 & 0.98 & 4, 13, 15 \\ \hline
R\(^2\) Franke & 0.917 & 0.92 & 0.908 & 5, 6, 9 \\ \hline
R\(^2\) Data & 0.103 & 0.103 & 0.102 & 4, 13, 15 \\ \hline
\end{tabular}
\caption{The lowest MSE and highest R\(^2\) estimate with corresponding polynomail degree from figure \ref{fig:optimal_r2_franke}, \ref{fig:optimal_r2_data}, \ref{fig:optimal_mse_franke} and \ref{fig:optimal_mse_data}. The \(\lambda\)-values used for ridge is \(\lambda = 0.002458\) and \(\lambda = 0.03091\) and for lasso \(\lambda = 7.2605\cdot 10^{-6}\) and \(\lambda = 3.7727\cdot 10^{-5}\) for Franke and data respectively.}
\label{tab:06}
\end{table}

Using the models for OLS, ridge and lasso with the best test data scores shown in table \ref{tab:06} i.e: 4\(^\text{th}\) order for OLS, 13\(^\text{th}\) order for ridge and 15\(^\text{th}\) order for lasso. The MSE and R\(^2\) error estimates for the entire data set for those models is shown in table \ref{tab:07}. When the models are created on the entire data set the error estimates are shown in table \ref{tab:07_2}. The ridge model is plotted in figure \ref{fig:ridge_3d} whereas the two remaining models from table \ref{tab:06} is plotted in figure \ref{fig:lasso_3d} and \ref{fig:ols_3d}.

\begin{table}
\begin{tabular}{|c|c|c|c|}\hline
Error estimates & OLS & Ridge & Lasso \\ \hline
MSE Franke & 0.009233 & 0.008609 & 0.009017 \\ \hline
MSE Data & 1.001 & 0.9985 & 1.001 \\ \hline
R\(^2\) Franke & 0.9023 & 0.9089 & 0.9046 \\ \hline
R\(^2\) Data & 0.09922 & 0.1012 & 0.09854 \\ \hline
\end{tabular}
\caption{The MSE and R\(^2\) scores for the entire data set with the best performing models from table \ref{tab:06} for OLS, ridge and lasso regression.}
\label{tab:07}
\end{table}

\begin{table}
\begin{tabular}{|c|c|c|c|}\hline
Error estimates & OLS & Ridge & Lasso \\ \hline
MSE Franke & 0.007161 & 0.006443 & 0.007304 \\ \hline
MSE Data & 0.9994 & 0.9971 & 1.001 \\ \hline
R\(^2\) Franke & 0.9242 & 0.9318 & 0.9227 \\ \hline
R\(^2\) Data & 0.1004 & 0.1025 & 0.09915 \\ \hline
\end{tabular}
\caption{The MSE and R\(^2\) scores for the entire data using the models with best performance in the test data errors from table \ref{tab:07} for OLS, ridge and lasso regression. The models are here used on the entire data grid not just the test or training section.}
\label{tab:07_2}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\textwidth]{ridge_3d_plot.png}
    \caption{A 3D plot of the 13\(^\text{th}\) order Ridge model with \(\lambda = 0.03091\) and the Franke function.}
    \label{fig:ridge_3d}
\end{figure}

The final figure of the Franke functon data is figure \ref{fig:test_vs_trainlarge_n} and this figure is a plot of the test and training MSE error from k-fold cross validation similar to figure \ref{fig:test_vs_train}. The only difference is that the data set used is of size \(400\times 200\). This calculation is also repeated N=50 times for random folds.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{tradeoff2MSEOLSlarge_n}
    \caption{A plot of the MSE from the test and training folds in k-fold cross validation with \(k=5\) folds for the Franke data using OLS. The values are the average of \(N=50\) runs with randomized folds. Unlike figure \ref{fig:test_vs_train} the data-set here is of size \(400\times 200\).}
    \label{fig:test_vs_trainlarge_n}
\end{figure}

\subsubsection{Terrain data}

Unlike the previous subsection I will not be including any tables about the \(\beta\)-parameters and their confidence intervals. All of this can be found in \texttt{terrain.py} in my github. 

A colormesh plot of the terrain data is shown in figure \ref{fig:terrain_data}. As in the figure and for the major parts of this subsection of the data set is down sampled from \(3600\times 1800\) to \(400\times 200\) if not stated otherwise. The downsampling is done by taking the average of \(9\times 9\) adjutant data points.

To determine the complexity of the model I will begin by finding the MSE and R\(^2\) scores by k-fold cross validation with 5-folds a total of \(N=10\) times for random folds. In figure \ref{fig:test_vs_train2} and \ref{fig:test_vs_train3} the mean MSE and R\(^2\) from k-fold run N-times are plotted against the used polynomial complexity. The error estimates are for the mean errors in the test and train data in k-fold. In both cases OLS is used.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{terrain_data.png}
    \caption{Colormesh plot of the terrain data used from file ``SRTM\_data\_Norway\_2" after being resampled to a size of \(400\times 200\).}
    \label{fig:terrain_data}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{tradeoff_terrain}
    \caption{A plot of the MSE from the test and training folds in k-fold cross validation with \(k=5\) folds for the terrain data using OLS. The values are the average of \(N=10\) runs with randomized folds.}
    \label{fig:test_vs_train2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{tradeoff_terrainR2}
    \caption{A plot of the R\(^2\) score from the test and training folds in k-fold cross validation with \(k=5\) folds for the terrain data using OLS. The values are the average of \(N=10\) runs with randomized folds.}
    \label{fig:test_vs_train3}
\end{figure}

Based on figure \ref{fig:test_vs_train2} and \ref{fig:test_vs_train3} there exist no ideal polynomial complexity where further increase in complexity would have negligible effect. Thus I will be choosing a complexity of maximum 15\(^\text{th}\) degree\footnote{I will later learn to regret this decision, so many hours running programs}. The average MSE and R\(^2\) error for \(N=10\) runs with random folds for a 15\(^\text{th}\) degree polynomial in k-fold with 5 folds can be found in table \ref{tab:08}.

\begin{table}
\begin{tabular}{|c|c|}\hline
Error estimates & OLS\\ \hline
MSE Test & 11677.3 \\ \hline
MSE Training & 11636.6 \\ \hline
R\(^2\) Test & 0.8657 \\ \hline
R\(^2\) Training & 0.8662 \\ \hline
\end{tabular}
\caption{The error estimates MSE and R\(^2\) from figure \ref{fig:test_vs_train2} and \ref{fig:test_vs_train3} for 15\(^\text{th}\) degree polynomial.}
\label{tab:08}
\end{table}

As done with the Franke function the data is splitted into training and test data. This split used to find the optimal \(\lambda\) parameter for ridge and lasso. The result of this is seen in figure \ref{fig:ridge_bad} and \ref{fig:lasso_bad} where the MSE in the test data is plotted against the corresponding \(\lambda\) parameter. Further increase in \(\lambda\)-values than demonstrated would prove pointless since it would not improve the accuracy of the model for a 15\(^\text{th}\) degree polynomial complexity. However for lower polynomial complexity \(\lambda > 0\) does in fact have a positive effect on the error estimates of the test data. 

In figure \ref{fig:lambda_pr_degree_terrain} the mean \(\lambda\) values which minimized the test error in k-fold with 4 folds ran \(N=50\) times is plotted against the model complexity. The error is the standard error \(\sigma/\sqrt{N}\) at \(95\%\) confidence of the mean \(\lambda\) from k-fold. Notice how the standard error doesn't take into account the number of folds used in k-fold. This is because the \(\lambda\) value is not the mean of the \(\lambda\)-values which minimized to error in each fold, but rather the \(\lambda\) where the sum of the test errors were minimized. In figure \ref{fig:lambda_pr_degree_terrain} where \(\lambda=0\) has not been plotted, thus stopping at 12 degree polynomials.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Ridge_bad}
    \caption{A plot of the mean MSE as a function of \(\lambda\) for the test folds in k-fold cross validation for \(k=5\) folds using ridge regression with the terrain data, with \(\lambda \in [10^{-9}, 10^{-7}]\) logarithmic spaced.}
    \label{fig:ridge_bad}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Lasso_bad}
    \caption{A plot of the mean MSE as a function of \(\lambda\) for the test folds in k-fold cross validation for \(k=5\) folds using lasso regression with the terrain data, with \(\lambda \in [10^{-11}, 10^{-9}]\) logarithmic spaced.}
    \label{fig:lasso_bad}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{best_lambda_terrain}
    \caption{The \(\lambda\)-values which minimizes the mean of the test errors in k-fold cross validation with 4-folds using ridge regression. The plotted \(\lambda\)-values are the mean of N = 50 k-fold runs with randomized folds pr complexity.}
    \label{fig:lambda_pr_degree_terrain}
\end{figure}

Using the \(\lambda\)-values from figure \ref{fig:lambda_pr_degree_terrain} and recreating figure \ref{fig:test_vs_train2} with ridge, the final result is plotted in figure \ref{fig:test_vs_train2ridge}. Further a comparison of the test MSE scores from ridge and OLS with the same folds is shown in table \ref{tab:09}. Note however that after 12\(^\text{th}\) polynomials \(\lambda = 0\) and differences are because of round off errors in the calculations.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{tradeoff_terrainridge}
    \caption{A plot of the MSE from the test and training folds in k-fold cross validation with \(k=5\) folds for the terrain data using Ridge regression with the optimal \(\lambda\) values from figure \ref{fig:lambda_pr_degree_terrain}. The values are the average of \(N=10\) runs with randomized folds.}
    \label{fig:test_vs_train2ridge}
\end{figure}

\begin{table}
\begin{tabular}{|c|c|c|}\hline
Complexity & OLS & Ridge\\ \hline
1 & 24382.158 & 24382.158 \\ \hline
2 & 20328.86 & 20328.86 \\ \hline
3 & 16819.655 & 16819.655 \\ \hline
4 & 15586.651 & 15586.651 \\ \hline
5 & 15035.038 & 15035.035 \\ \hline
6 & 14499.407 & 14499.4 \\ \hline
7 & 14119.283 & 14119.277 \\ \hline
8 & 13723.398 & 13723.39 \\ \hline
9 & 13449.225 & 13449.216 \\ \hline
10 & 13045.589 & 13045.57 \\ \hline
11 & 12804.559 & 12804.518 \\ \hline
12 & 12361.396 & 12361.394 \\ \hline
13 & 12035.657 & 12035.657 \\ \hline
14 & 11890.92 & 11890.92 \\ \hline
15 & 11677.324 & 11677.324 \\ \hline
16 & 11317.505 & 11317.505 \\ \hline
17 & 10927.481 & 10927.481 \\ \hline
18 & 10664.592 & 10664.591 \\ \hline
19 & 10478.644 & 10478.645 \\ \hline
20 & 10343.452 & 10343.45 \\ \hline
\end{tabular}
\caption{The error estimates for the MSE in the test data from figure\ref{fig:test_vs_train2} and \ref{fig:test_vs_train2ridge} for all polynomials from 1 to 20.}
\label{tab:09}
\end{table}

Using a 15\(^\text{th}\) degree complexity to create the model the MSE and R\(^2\) error estimates for the entire data set is shown in table \ref{tab:10} and the inevitable error is \(\sigma = 107.98423\). Using a simple train test split the error estimates is shown in table \ref{tab:11} and the inevitable error for the train test case is \(\sigma = 107.98418\).

\begin{table}
\begin{tabular}{|c|c|c|c|}\hline
Error estimates & OLS & Ridge & Lasso\\ \hline
MSE Data & 11641.0 & 12700.0 & 15349.0 \\ \hline
R\(^2\) Data & 0.86612 & 0.85393 & 0.82346 \\ \hline
\end{tabular}
\caption{The error estimates MSE and R\(^2\) from figure \ref{fig:test_vs_train2} and \ref{fig:test_vs_train3} for 15\(^\text{th}\) degree polynomial.}
\label{tab:10}
\end{table}

\begin{table}
\begin{tabular}{|c|c|c|c|}\hline
Error estimates & OLS & Ridge & Lasso\\ \hline
MSE Data & 11632.0 & 12722.0 & 15305.0 \\ \hline
R\(^2\) Data & 0.86617 & 0.85363 & 0.82392 \\ \hline
MSE Data & 11676.0 & 12813.0 & 15467.0 \\ \hline
R\(^2\) Data & 0.86581 & 0.85274 & 0.82224 \\ \hline
\end{tabular}
\caption{The error estimates MSE and R\(^2\) for a OLS model created using the entire terrain data set with a 15\(^\text{th}\) degree complexity. The \(\lambda\)-values used for ridge and lasso regression is \(\lambda = 10^{-8}\) for both.}
\label{tab:11}
\end{table}

Using k-fold cross validation on the entire data set with \(k=5\) folds the MSE and R\(^2\) error estimates is given in table \ref{tab:12}, and figure \ref{fig:hist_terrain_ols} is a histogram of MSE for k-fold cross validation with 10 folds for OLS. The same histogram but for ridge and lasso regression with \(\lambda = 10^{-8}\) is shown in figure \ref{fig:hist_terrain_ridge} and \ref{fig:hist_terrain_lasso}.

\begin{table}
\begin{tabular}{|c|c|c|c|}\hline
Error estimates & OLS & Ridge & Lasso\\ \hline
MSE Data & 11700.0 \(\pm\) 362.0 & 12700.0 \(\pm\) 463.0 & 15400.0 \(\pm\) 357.0 \\ \hline
R\(^2\) Data & 0.866 \(\pm\) 0.00471 & 0.853 \(\pm\) 0.00533 & 0.823 \(\pm\) 0.00383 \\ \hline
MSE Data & 11600.0 \(\pm\) 90.1 & 12700.0 \(\pm\) 113.0 & 15300.0 \(\pm\) 94.5 \\ \hline
R\(^2\) Data & 0.866 \(\pm\) 0.00117 & 0.854 \(\pm\) 0.00131 & 0.823 \(\pm\) 0.00099 \\ \hline
\end{tabular}
\caption{The mean error estimates MSE and R\(^2\) for the test data with OLS, ridge and lasso using k-fold cross validation with \(k=10\) folds. The \(\lambda\)-values used for ridge and lasso regression is \(\lambda = 10^{-8}\) for both. The error shown is the standard deviation with a confidence of \(95\%\).}
\label{tab:12}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{terrainHist_Olsk=10.png}
    \caption{The MSE for the test folds in k-fold cross validation with \(k=10\) folds using OLS.}
    \label{fig:hist_terrain_ols}
\end{figure}

Using 15\(^{th}\) degree complexity for the entire data set a colormap of the model is shown in figure \ref{fig:colormap_terrain_model}. For comparison a 5\(^\text{th}\) degree polynomial fit has the following error estimates \(MSE = 15026\) and \(R^2 = 0.8272\) and the colorplot is shown in figure \ref{fig:colormap_terrain_model5th}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{colormap_terrain.png}
    \caption{A colormap of the model created using 15\(^\text{th}\) degree polynomial complexity with OLS on the entire data set.}
    \label{fig:colormap_terrain_model}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{colormap_terrain5th.png}
    \caption{A colormap of the model created using 5\(^\text{th}\) degree polynomial complexity with OLS on the entire data set.}
    \label{fig:colormap_terrain_model5th}
\end{figure}

\section{Discussion}\label{sec:Discussion}

\subsubsection{Regression methods}

In this subsection I will be comparing the different regression methods not the data sets themselves. This will be carried out in a later subsection.

As expected the OLS models outperformed both ridge regression and lasso regression when fitting the entire data set, and evaluating the models on the same set the models were created. This should come as no surprise since the derivation of OLS is based around minimizing the cost function. From the vector space derivation the OLS model is the closest point in the vector space spanned by \(\bm{X}\) to the data set. It can be seen from table \ref{tab:02} and table \ref{tab:03} where the error was smaller than both ridge and lasso.

The problem where OLS may struggle is when you want a predictive model. OLS is very sensitive to over fitting as seen in figure \ref{fig:test_vs_train}. This is however less a problem with a larger data set as seen in figure \ref{fig:test_vs_trainlarge_n}. Ridge and lasso are far less sensitive as seen in figure \ref{fig:optimal_r2_franke} and \ref{fig:optimal_r2_data}. It's clear that their strengths lies in the fact that higher order complexity can be utilized without overfitting. As an extension they are also more stable when the data consist of fewer data points. 

In my case ridge regression gave constantly better results for both the Franke function and the terrain data than lasso regression did. This may have something to do with how they shrink the \(\beta\) coefficients. As seen in table \ref{tab:02} lasso regression has a tendency to make coefficients zero and way smaller than ridge with larger \(\lambda\)-values. Therefore lasso regression may be even more stable than ridge for even higher complexity than tested. Figure \ref{fig:optimal_r2_data} and \ref{fig:optimal_r2_franke} may seem to indicate such an observation, since the lasso were either stable or improving while ridge where either stable or degrading.

So far I have only covered how the regression methods behaved for a small data sets. As evident in figure \ref{fig:test_vs_trainlarge_n} OLS is far more stable for a higher number of data points n. This is also very evident in figure \ref{fig:test_vs_train2} where there is almost no difference between the test and training error. Further from figure \ref{fig:ridge_bad} and \ref{fig:lasso_bad} it's clear that in these cases ridge and lasso regression would only worsen the result. This may simply be because the strengths of ridge and lasso regression is that it can utilize higher degree polynomial complexity than OLS without over-fitting. 

An interesting result is shown in figure \ref{fig:lambda_pr_degree_terrain} where \(\lambda \rightarrow 0\) for larger polynomial complexity. This is unexpected as I would expect the ideal \(\lambda\)-values to increase as it does with the Franke function data as seen in figure \ref{fig:optimal_lambdas} to keep the model stable. My guess for this is that the data is to advanced for lower polynomials give good estimates, and thus changing \(\lambda\) would be similar to tilting a plane. For more advanced models less and less tilting is required. This is however just a wild guess. It is however clear that the polynomial order is too low to properly make use of the stabilizing qualities of ridge and lasso. Thus making OLS a better option since my used polynomials are rather stable without shrinking.

As a short summary, OLS is the better option for fitting entire data sets or when the complexity and number of data points are high. Ridge and lasso are better options when there are fewer data points and OLS begins to overfit the data since the shrinkage keeps the model stable. For very high complexity, lasso may be the better option than ridge. This is because the shrinkage of \(\lambda\)-values are greater for lasso than for ridge as seen in table \ref{tab:02}, and figure \ref{fig:ridge_bad} and \ref{fig:lasso_bad} shows a more positive trend for lasso than for ridge at higher complexity.

\subsubsection{Data sets}

So far the regression methods haven been the focal point. In this subsection the data sets and their best models will be in focus.

We will begin with the terrain data. It's very clear that from figure \ref{fig:test_vs_train2} that there is very little overfitting when using OLS. This is further emphasized in table \ref{tab:09} where the difference between OLS and ridge regression with the ideal \(\lambda\) are practically non existent. Although I will say that ridge regression is smaller from 4\(^\text{th}\) degree up to 12\(^\text{th}\) degree polynomials. After this all ideal \(lambda\) values were 0. There is however no point in using so low polynomial degrees since higher order are complexity is objectively better as seen by the MSE. Especially since there are no noticeable overfitting. By comparing figure \ref{fig:terrain_data} with the models in figure \ref{fig:colormap_terrain_model} and \ref{fig:colormap_terrain_model5th} there is clear that the 15\(^\text{th}\) degree complexity is far more capable of reproducing the original surface.

Lasso regression is even worse than ridge regression in fitting the terrain data. Unlike ridge which gave some (very very minor) improvements in the test error for low polynomial degrees, lasso is objectively the worst option. This is evident in figure \ref{fig:lasso_bad} where even \(\lambda = 0\) gave worse error estimates than OLS.

The terrain data was simply to complex and had to many data points such that OLS could not over-fit. Because of the lack of overfitting ridge and lasso lost the advantage of higher order complexity.

For the Frank function the linear regression which gave the best fit is not as simple as for the terrain data. Unlike the for the terrain data, lasso regression out preformed OLS in one specific case.

I will here look at table \ref{tab:06}, \ref{tab:07} and \ref{tab:07_2}. This is simply because the data shown in table \ref{tab:07} and \ref{tab:07_2} are based on model I would have chosen. This is because these models are the models which minimized the error estimate for the test data in table \ref{tab:06}. It is worth noting that ridge outperformed both lasso regression and OLS while lasso outperformed OLS with respect to real Franke function in table \ref{tab:07_2}, however OLS generally outperformed Lasso. A general trend was that ridge regression gave better results when used in predictions wile OLS gave the better result when used on entire data set. There is however worth noting that the difference between the error estimates for all regression methods are very similar.

All regression methods give good results in reproducing the original data, and a point could be made for all of the methods. I would however make a point in noting that finding the optimal \(\lambda\)-values for ridge and lasso is far computationally demanding than using OLS. Further the extra complexity need places more emphasis the computational cost of these two methods. Albeit I believe from table \ref{tab:07} and figure \ref{fig:optimal_lambdas} that ridge and lasso may be safer options to avoid overfitting, the extra computational power need does make OLS seem like the better option. Time used on finding \(\lambda\) may instead be used on finding the ideal polynomial complexity to avoid overfitting, but if the end goal is to make predictive focused models ridge and lasso seems like the better option.

\subsubsection{Bias-variance tradeoff}

I will end this section as a whole with a small discussion about the bias-variance tradeoff. This part will not resolve around the regression methods but the over-fitting itself.

To do this figure \ref{fig:test_vs_train} and \ref{fig:bias_varianceOLS} will play a central role. As seen in figure \ref{fig:test_vs_train} the training and test data starts diverging. The reason for this is that the model starts fitting not only to the underlying function, but to the noise of the data set. As seen in figure \ref{fig:bias_varianceOLS} the variance between multiple predicted models with re randomized noise is increasing, while the bias squared is decreasing. The ideal model would than be when the sum of the bias and variance terms are at the smallest. This is in great agreement with my result showing that polynomials for OLS has smallest error between 4 and 6.

For ridge and lasso the bias-variance curves would differ. They impose a penalty on the coefficients and thus reducing the variance term. This will however impact the bias term in comparison to OLS for the same degree. Thus ridge and lasso is based around the principle that the gain in bias is less than the reduction in variance.

\section{Conclusion}\label{sec:Conclusion}

OLS outperforms both ridge and lasso regression for larger more complex data sets such as the terrain data. Here the \(MSE=11677\) for a polynomial of 15\(^\text{th}\) degree complexity. For lasso regression the same error estimate gave at best \(MSE\approx 15000\) for \(\lambda = 0\), whereas for ridge only very small \(\lambda\)-values could give result near that of OLS.

For the smaller data set such as the Franke function OLS has a tendency to over-fit for reasonably low complexity. This is seen from the bias-variance tradeoff where the ideal polynomial degree is around 5\(^\text{th}\). In these cases the shrinkage for \(\beta\)-parameters in ridge and lasso are more stable and are less prone to overfitting. They therefore works better with higher complexity. When predicting data a high complexity ridge model with \(MSE = 0.0086\) and a high complexity lasso model with \(MSE = 0.009\) outperformed a 4\(\text{th}\) order OLS model with \(MSE = 0.0092\) when predicting on the real Franke function.

Therefore my conclusion is that OLS works best for larger data sets, while ridge and lasso works best for smaller data sets where overfitting may be problematic.

\onecolumngrid
\newpage
\twocolumngrid
\appendix

\section{Tables}

\begin{table}[H]
\begin{tabular}{|c|c|c|c|}\hline
\(\beta\) & OLS & Ridge & Lasso \\ \hline
\(\beta_{0}\) & 0.457 \(\pm\) 0.469 & 0.699 \(\pm\) 0.285 & 1.05 \(\pm\) 0.437 \\ \hline
\(\beta_{1}\) & 6.04 \(\pm\) 5.45 & 3.16 \(\pm\) 1.94 & -0.0837 \(\pm\) 4.86 \\ \hline
\(\beta_{2}\) & 5.09 \(\pm\) 5.38 & 2.92 \(\pm\) 1.92 & 0.555 \(\pm\) 4.79 \\ \hline
\(\beta_{3}\) & -23.1 \(\pm\) 26.8 & -12.5 \(\pm\) 5.22 & -2.43 \(\pm\) 23.4 \\ \hline
\(\beta_{4}\) & -5.23 \(\pm\) 20.2 & 1.59 \(\pm\) 4.5 & 1.81 \(\pm\) 18.4 \\ \hline
\(\beta_{5}\) & -23.2 \(\pm\) 27.3 & -13.2 \(\pm\) 5.24 & -4.53 \(\pm\) 23.7 \\ \hline
\(\beta_{6}\) & 15.3 \(\pm\) 61.2 & 6.07 \(\pm\) 5.82 & 0.639 \(\pm\) 53.0 \\ \hline
\(\beta_{7}\) & 38.0 \(\pm\) 44.5 & 8.66 \(\pm\) 6.47 & 3.29 \(\pm\) 40.4 \\ \hline
\(\beta_{8}\) & -2.12 \(\pm\) 43.1 & -6.93 \(\pm\) 6.5 & -3.63 \(\pm\) 39.3 \\ \hline
\(\beta_{9}\) & 29.4 \(\pm\) 63.1 & 9.71 \(\pm\) 5.75 & 2.27 \(\pm\) 54.3 \\ \hline
\(\beta_{10}\) & 16.1 \(\pm\) 64.8 & 9.88 \(\pm\) 6.5 & 0.875 \(\pm\) 56.5 \\ \hline
\(\beta_{11}\) & -59.4 \(\pm\) 48.7 & -9.13 \(\pm\) 8.5 & 0.0 \(\pm\) 44.9 \\ \hline
\(\beta_{12}\) & 21.7 \(\pm\) 44.4 & 11.1 \(\pm\) 9.01 & 0.0 \(\pm\) 41.1 \\ \hline
\(\beta_{13}\) & -24.2 \(\pm\) 46.8 & -6.5 \(\pm\) 8.68 & -0.00225 \(\pm\) 43.4 \\ \hline
\(\beta_{14}\) & -5.41 \(\pm\) 66.7 & 8.73 \(\pm\) 6.38 & 1.56 \(\pm\) 57.7 \\ \hline
\(\beta_{15}\) & -14.8 \(\pm\) 25.7 & -7.35 \(\pm\) 4.43 & -0.00429 \(\pm\) 22.7 \\ \hline
\(\beta_{16}\) & 23.6 \(\pm\) 22.0 & 0.152 \(\pm\) 6.9 & -2.27 \(\pm\) 20.7 \\ \hline
\(\beta_{17}\) & 0.97 \(\pm\) 21.1 & -1.3 \(\pm\) 9.03 & 0.0 \(\pm\) 20.1 \\ \hline
\(\beta_{18}\) & -13.5 \(\pm\) 21.1 & -4.3 \(\pm\) 9.09 & 1.05 \(\pm\) 20.2 \\ \hline
\(\beta_{19}\) & 19.9 \(\pm\) 21.2 & 6.43 \(\pm\) 6.88 & 0.0 \(\pm\) 20.1 \\ \hline
\(\beta_{20}\) & -5.79 \(\pm\) 26.3 & -8.38 \(\pm\) 4.45 & -0.216 \(\pm\) 23.0 \\ \hline
\end{tabular}
\caption{The \(\beta\)-values for the OLS, Ridge and Lasso regression models on the training data with their respective confidence interval at \(95\%\). For Ridge \(\lambda = 4.95\cdot 10^{-3}\) while for Lasso \(\lambda = 6.34\cdot 10^{-5}\).}
\label{tab:13}
\end{table}

\begin{table}[H]
\begin{tabular}{|c|c|}\hline
\(\beta\) & OLS \\ \hline
\(\beta_{0}\) & 0.375 \(\pm\) 0.0435 \\ \hline
\(\beta_{1}\) & 8.04 \(\pm\) 0.535 \\ \hline
\(\beta_{2}\) & 4.28 \(\pm\) 0.536 \\ \hline
\(\beta_{3}\) & -32.0 \(\pm\) 2.64 \\ \hline
\(\beta_{4}\) & -10.3 \(\pm\) 2.21 \\ \hline
\(\beta_{5}\) & -13.9 \(\pm\) 2.44 \\ \hline
\(\beta_{6}\) & 35.9 \(\pm\) 5.76 \\ \hline
\(\beta_{7}\) & 40.2 \(\pm\) 4.49 \\ \hline
\(\beta_{8}\) & 4.61 \(\pm\) 4.73 \\ \hline
\(\beta_{9}\) & 4.92 \(\pm\) 5.05 \\ \hline
\(\beta_{10}\) & -6.54 \(\pm\) 5.79 \\ \hline
\(\beta_{11}\) & -50.2 \(\pm\) 5.05 \\ \hline
\(\beta_{12}\) & 8.06 \(\pm\) 5.84 \\ \hline
\(\beta_{13}\) & -19.2 \(\pm\) 4.74 \\ \hline
\(\beta_{14}\) & 18.2 \(\pm\) 4.98 \\ \hline
\(\beta_{15}\) & -5.8 \(\pm\) 2.23 \\ \hline
\(\beta_{16}\) & 17.4 \(\pm\) 2.38 \\ \hline
\(\beta_{17}\) & 4.79 \(\pm\) 1.96 \\ \hline
\(\beta_{18}\) & -10.5 \(\pm\) 2.93 \\ \hline
\(\beta_{19}\) & 14.8 \(\pm\) 2.48 \\ \hline
\(\beta_{20}\) & -13.6 \(\pm\) 1.92 \\ \hline
\end{tabular}
\caption{The \(\beta\)-values for the OLS regression method using 10 folds in k-fold cross validation on the training data in a train/test data set. The confidence interval is 95\%, and is standard error between the corresponding \(\beta_i\)-coefficients i.e the std for \(\beta_0\), \(\beta_1\), \(\beta_2\) etc.}
\label{tab:14}
\end{table}

\section{Figures}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Ridge_and_largelamba}
    \caption{The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda\) values. It's the same plot as figure \ref{fig:Vary_lambda_R} but for larger \(\lambda\) values i.e \(\lambda \in [0, 10]\) logarithmic spaced.}
    \label{fig:Vary_lambda_R2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Lasso_and_largelambda}
    \caption{The MSE between the test set and the predicted model using Ridge regression for different polynomial orders and \(\lambda\) values. It's the same plot as figure \ref{fig:Vary_lambda_L} but for larger \(\lambda\) values i.e \(\lambda \in [0, 0.1]\) logarithmic spaced with 1001 iterations.}
    \label{fig:Vary_lambda_L2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{ridge_lasso_high_order_polyMSE_data}
    \caption{The MSE between a test set from the data and the regression methods OLS, ridge and lasso using the ''optimal'' \(\lambda\)-values for each polynomial degree and method from figure \ref{fig:optimal_lambdas}.}
    \label{fig:optimal_mse_franke}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{ridge_lasso_high_order_polyMSE}
    \caption{The MSE between a test set and the regression methods OLS, ridge and lasso using the ''optimal'' \(\lambda\)-values for each polynomial degree and method from figure \ref{fig:optimal_lambdas}. The test set here is for the real Franke function without noise, but it correspond to the part of the data set omitted in training the model.}
    \label{fig:optimal_mse_data}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{ridge_lasso_lambda_poly}
    \caption{The ideal \(\lambda\) values for the Franke function.}
    \label{fig:optimal_lambdas}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{ols_3d_plot.png}
    \caption{A 3D plot of the 4\(^\text{th}\) order OLS model from table \ref{tab:07} beside the Franke function.}
    \label{fig:ols_3d}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{lasso_3d_plot.png}
    \caption{A 3D plot of the 15\(^\text{th}\) order Lasso model from table \ref{tab:07} with \(\lambda = 3.7727\cdot 10^{-5}\) beside the Franke function.}
    \label{fig:lasso_3d}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{terrainHist_Ridgek=10.png}
    \caption{The MSE for the test folds in k-fold cross validation with \(k=10\) folds for the terrain data and two different \(\lambda\)-values in ridge regression.}
    \label{fig:hist_terrain_ridge}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{terrainHist_Lassok=10.png}
    \caption{The MSE for the test folds in k-fold cross validation with \(k=10\) folds for the terrain data and two different \(\lambda\)-values in lasso regression.}
    \label{fig:hist_terrain_lasso}
\end{figure}

\onecolumngrid
\section{Proofs}\label{sec:proof}

\subsubsection{Bias-variance tradeoff proof}

\begin{align*}
\mathbb{E}[(\bm{y}-\bm{\tilde{y}})^2]& = E[(f(\bm{x})+\bm{\epsilon}-\bm{\tilde{y}})^2] \\
&= \mathbb{E}[(f(\bm{x})-\bm{\tilde{y}})^2]+2\mathbb{E}[(f(\bm{x})-\bm{\tilde{y}})\bm{\epsilon}]+\mathbb{E}[\bm{\epsilon}^2]\\
&= \mathbb{E}[(f(\bm{x})-\bm{\tilde{y}})^2]+2\mathbb{E}[(f(\bm{x})-\bm{\tilde{y}})]\mathbb{E}[\bm{\epsilon}]+\sigma^2 \\
&= \mathbb{E}[(f(\bm{x})-\bm{\tilde{y}})^2] + \sigma^2 \\
&= \mathbb{E}[(f(\bm{x}) + \mathbb{E}[\bm{\tilde{y}}] - \mathbb{E}[\bm{\tilde{y}}] - \bm{\tilde{y}})^2] + \sigma^2\\
&= \left[f(\bm{x}) - \mathbb{E}[\bm{\tilde{y}}] \right]^2 + \mathbb{E}\left[\bm{\tilde{y}} - \mathbb{E}[\bm{\tilde{y}}] \right]^2 + \sigma^2\\
&= \left[f(\bm{x}) - \mathbb{E}[\bm{\tilde{y}}] \right]^2 + \mathbb{E}\left[\bm{\tilde{y}} - \mathbb{E}[\bm{\tilde{y}}] \right]^2 + \sigma^2\\
&= \text{Bias}^2(\bm{\tilde{y}}) + \text{Var}(\bm{\tilde{y}}) +  \sigma^2.
\end{align*}
A couple small comments to the above calculations. For two independent variables \(\bm{x}\) and \(\bm{y}\) the expectation value goes as follows
\begin{equation}
\mathbb{E}[\bm{xy}] = \mathbb{E}[\bm{x}]\mathbb{E}[\bm{y}].
\end{equation}
This is the case for \(\bm{\epsilon}\), \(\bm{\tilde{y}}\) and \(f(\bm{x})\). Therefore
\begin{equation}
2\mathbb{E}[(f(\bm{x})-\bm{\tilde{y}})\bm{\epsilon}] = 2\mathbb{E}[(f(\bm{x})-\bm{\tilde{y}})]\mathbb{E}[\bm{\epsilon}] = 0
\end{equation}
since \(\mathbb{E}[\bm{\epsilon}] = 0\). Another unclear term is the following
\begin{align}
2\mathbb{E}\left[ \left(f(\bm{x}) + \mathbb{E}[\bm{\tilde{y}}]\right)\left( \bm{\tilde{y}} - \mathbb{E}[\bm{\tilde{y}}]\right) \right] = 0
\end{align}
which I omitted writing in line six because it's zero. This is zero because
\begin{align}
\mathbb{E}\left[ \bm{\tilde{y}} - \mathbb{E}[\bm{\tilde{y}}]\right] &= \mathbb{E}[\bm{\tilde{y}}] - \mathbb{E}[\mathbb{E}[\bm{\tilde{y}}]]\\
&= \mathbb{E}[\bm{\tilde{y}}] - \mathbb{E}[\bm{\tilde{y}}] = 0.
\end{align}

\bibliographystyle{plain}
\bibliography{references}

\end{document}