\relax 
\providecommand\hyper@newdestlabel[2]{}
\bbl@beforestart
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{norsk}{}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{Logistic regression and Neural Networks}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {abstract}{Sammendrag}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}\protected@file@percent }
\newlabel{sec:Introduction}{{I}{1}{}{section*.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Theory}{1}{section*.4}\protected@file@percent }
\newlabel{sec:Theory}{{II}{1}{}{section*.4}{}}
\newlabel{eq:01}{{II.1}{1}{}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Preprocessing and sampling}{1}{section*.5}\protected@file@percent }
\citation{MHJ_GD}
\newlabel{eq:02}{{II.2}{2}{}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Gradient decent}{2}{section*.6}\protected@file@percent }
\newlabel{sec:gd}{{II\tmspace  +\thinmuskip {.1667em}B}{2}{}{section*.6}{}}
\newlabel{eq:sd}{{II.3}{2}{}{equation.2.3}{}}
\citation{MHJ_LogReg}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient descent\relax }}{3}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:01}{{1}{3}{Gradient descent\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Logistic regression}{3}{section*.7}\protected@file@percent }
\newlabel{eq:03}{{II.10}{3}{}{equation.2.10}{}}
\newlabel{eq:04}{{II.12}{3}{}{equation.2.12}{}}
\newlabel{eq:05}{{II.13}{3}{}{equation.2.13}{}}
\newlabel{eq:06}{{II.14}{3}{}{equation.2.14}{}}
\newlabel{eq:07}{{II.15}{3}{}{equation.2.15}{}}
\newlabel{eq:08}{{II.16}{3}{}{equation.2.16}{}}
\citation{MHJ_NN}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Neural Network}{4}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Feed forward}{4}{section*.9}\protected@file@percent }
\newlabel{eq:z_l}{{II.18}{4}{}{equation.2.18}{}}
\newlabel{eq:09}{{II.19}{4}{}{equation.2.19}{}}
\newlabel{eq:new_eq}{{II.20}{4}{}{equation.2.20}{}}
\newlabel{eq:10}{{II.21}{4}{}{equation.2.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An illustration of a neural network with the corresponding equation connecting each layer.\relax }}{4}{figure.caption.10}\protected@file@percent }
\newlabel{fig:01}{{1}{4}{An illustration of a neural network with the corresponding equation connecting each layer.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Back propagation}{5}{section*.11}\protected@file@percent }
\newlabel{eq:11}{{II.22}{5}{}{equation.2.22}{}}
\newlabel{eq:12}{{II.23}{5}{}{equation.2.23}{}}
\newlabel{eq:c_entropy}{{II.25}{5}{}{equation.2.25}{}}
\newlabel{eq:13}{{II.28}{5}{}{equation.2.28}{}}
\citation{wiki:F1_score}
\citation{wiki:F1_score}
\newlabel{eq:14}{{II.34}{6}{}{equation.2.34}{}}
\newlabel{eq:delta_L}{{II.37}{6}{}{equation.2.37}{}}
\newlabel{eq:partial_c}{{II.38}{6}{}{equation.2.38}{}}
\newlabel{eq:delta_l}{{II.40}{6}{}{equation.2.40}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Backpropagation(GD = gradient descent)\relax }}{6}{algorithm.2}\protected@file@percent }
\newlabel{alg:02}{{2}{6}{Backpropagation(GD = gradient descent)\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Model evaluation}{6}{section*.12}\protected@file@percent }
\newlabel{eq:15}{{II.41}{6}{}{equation.2.41}{}}
\citation{yeh2009comparisons}
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{7}{section*.13}\protected@file@percent }
\newlabel{sec:Method}{{III}{7}{}{section*.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Code implementation}{7}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Logistic regression}{7}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Neural Network}{7}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Preparations}{7}{section*.17}\protected@file@percent }
\newlabel{sec:prep}{{III\tmspace  +\thinmuskip {.1667em}B}{7}{}{section*.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Credit card data}{7}{section*.18}\protected@file@percent }
\citation{Jon_P1}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Franke function}{8}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Logistic regression}{8}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Neural Network Credit card}{9}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Neural Network Franke}{9}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{10}{section*.23}\protected@file@percent }
\newlabel{sec:Results}{{IV}{10}{}{section*.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Logistic regression}{10}{section*.24}\protected@file@percent }
\newlabel{eq:16}{{IV.1}{11}{}{equation.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The accuracy and F1 scores of logistic regression model as a function of the ratio \(\frac  {S_0}{S_1}\) i.e the ratio between 0-samples and 1-samples in the training data. The batch size is 200, a learning rate 0.001 and 1000 epochs.\relax }}{11}{figure.caption.25}\protected@file@percent }
\newlabel{fig:02}{{2}{11}{The accuracy and F1 scores of logistic regression model as a function of the ratio \(\frac {S_0}{S_1}\) i.e the ratio between 0-samples and 1-samples in the training data. The batch size is 200, a learning rate 0.001 and 1000 epochs.\relax }{figure.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces The accuracy scores of the logistic regression model for different learning rates and epoch, with a constant batch size of 200 with the down-sampling ratio given in \ref  {eq:16}.\relax }}{11}{table.caption.26}\protected@file@percent }
\newlabel{tab:01}{{I}{11}{The accuracy scores of the logistic regression model for different learning rates and epoch, with a constant batch size of 200 with the down-sampling ratio given in \ref {eq:16}.\relax }{table.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The F1 scores of the logistic regression model for different learning rates and epoch, with a constant batch size of 200 with the down-sampling ratio given in \ref  {eq:16}.\relax }}{11}{table.caption.27}\protected@file@percent }
\newlabel{tab:02}{{II}{11}{The F1 scores of the logistic regression model for different learning rates and epoch, with a constant batch size of 200 with the down-sampling ratio given in \ref {eq:16}.\relax }{table.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The accuracy, F1, precision, recall as a function of the threshold used in determining whether a probability corresponds to a 0 or 1 sample. This is for a 0.0001 learning rate, 1000 epochs, 200 batch size model with a down-sampling ratio of \(\frac  {S_0}{S_1} \approx 1.6\).\relax }}{12}{figure.caption.28}\protected@file@percent }
\newlabel{fig:03}{{3}{12}{The accuracy, F1, precision, recall as a function of the threshold used in determining whether a probability corresponds to a 0 or 1 sample. This is for a 0.0001 learning rate, 1000 epochs, 200 batch size model with a down-sampling ratio of \(\frac {S_0}{S_1} \approx 1.6\).\relax }{figure.caption.28}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces The Accuracy scores of the logistic regression model for different learning rates and epoch, with a constant batch size of 200 with the down-sampling ratio of 1.\relax }}{12}{table.caption.29}\protected@file@percent }
\newlabel{tab:03}{{III}{12}{The Accuracy scores of the logistic regression model for different learning rates and epoch, with a constant batch size of 200 with the down-sampling ratio of 1.\relax }{table.caption.29}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces The F1 scores of the logistic regression model for different learning rates and epoch, with a constant batch size of 200 with the down-sampling ratio of 1.\relax }}{12}{table.caption.30}\protected@file@percent }
\newlabel{tab:04}{{IV}{12}{The F1 scores of the logistic regression model for different learning rates and epoch, with a constant batch size of 200 with the down-sampling ratio of 1.\relax }{table.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The accuracy, F1, precision, recall as a function of the threshold used in determining whether a probability corresponds to a 0 or 1 sample. This is for a 0.0001 learning rate, 1000 epochs, 200 batch size model with a down-sampling ratio of \(\frac  {S_0}{S_1} = 1\).\relax }}{12}{figure.caption.31}\protected@file@percent }
\newlabel{fig:04}{{4}{12}{The accuracy, F1, precision, recall as a function of the threshold used in determining whether a probability corresponds to a 0 or 1 sample. This is for a 0.0001 learning rate, 1000 epochs, 200 batch size model with a down-sampling ratio of \(\frac {S_0}{S_1} = 1\).\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Neural Network}{12}{section*.32}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces The accuracy for a NN with tanh activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }}{13}{table.caption.33}\protected@file@percent }
\newlabel{tab:05}{{V}{13}{The accuracy for a NN with tanh activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }{table.caption.33}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces The F1 for a NN with tanh activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }}{13}{table.caption.34}\protected@file@percent }
\newlabel{tab:06}{{VI}{13}{The F1 for a NN with tanh activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }{table.caption.34}{}}
\newlabel{eq:17}{{IV.6}{13}{}{equation.4.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The accuracy and F1 scores for a NN with a hidden layer with 64 nodes and tanh activation as a function of the ratio \(\frac  {S_0}{S_1}\) i.e the ratio between 0-samples and 1-samples in the training data. The batch size is 200, a learning rate of 0.0001 and 100 epochs.\relax }}{13}{figure.caption.35}\protected@file@percent }
\newlabel{fig:05}{{5}{13}{The accuracy and F1 scores for a NN with a hidden layer with 64 nodes and tanh activation as a function of the ratio \(\frac {S_0}{S_1}\) i.e the ratio between 0-samples and 1-samples in the training data. The batch size is 200, a learning rate of 0.0001 and 100 epochs.\relax }{figure.caption.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces The best accuracy, F1 and norm between them for both 1 and 2 hidden layers for the given activation function and number of nodes with 100 epochs, a learning rate of 0.0001 and a batch size of 200. The ratio of the down-sampling used is given in \ref  {eq:17}.\relax }}{13}{table.caption.36}\protected@file@percent }
\newlabel{tab:07}{{VII}{13}{The best accuracy, F1 and norm between them for both 1 and 2 hidden layers for the given activation function and number of nodes with 100 epochs, a learning rate of 0.0001 and a batch size of 200. The ratio of the down-sampling used is given in \ref {eq:17}.\relax }{table.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A histogram of all the F1 scores for all the models with different activation functions and nodes with one hidden layer for 100 epochs, batch size of 200 and 0.0001 learning rate. The ratio of the down-sampling used is given in \ref  {eq:17}.\relax }}{14}{figure.caption.37}\protected@file@percent }
\newlabel{fig:06}{{6}{14}{A histogram of all the F1 scores for all the models with different activation functions and nodes with one hidden layer for 100 epochs, batch size of 200 and 0.0001 learning rate. The ratio of the down-sampling used is given in \ref {eq:17}.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A histogram of all the F1 scores for all the models with different activation functions and nodes with two hidden layers for 100 epochs, batch size of 200 and 0.0001 learning rate. The ratio of the down-sampling used is given in \ref  {eq:17}.\relax }}{14}{figure.caption.38}\protected@file@percent }
\newlabel{fig:07}{{7}{14}{A histogram of all the F1 scores for all the models with different activation functions and nodes with two hidden layers for 100 epochs, batch size of 200 and 0.0001 learning rate. The ratio of the down-sampling used is given in \ref {eq:17}.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A histogram of the F1 scores for the same model but with different initial random weights with the mean and standard deviation for a 74 tanh - 104 sigmoid model (the same as in table \ref  {tab:07}).\relax }}{14}{figure.caption.39}\protected@file@percent }
\newlabel{fig:08}{{8}{14}{A histogram of the F1 scores for the same model but with different initial random weights with the mean and standard deviation for a 74 tanh - 104 sigmoid model (the same as in table \ref {tab:07}).\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The accuracy of a one hidden and two hidden layer models (the best scoring F1 models from table \ref  {tab:07}) with different \(\lambda \)-values. The initial weights are the same for all \(\lambda \)-values.\relax }}{15}{figure.caption.40}\protected@file@percent }
\newlabel{fig:09}{{9}{15}{The accuracy of a one hidden and two hidden layer models (the best scoring F1 models from table \ref {tab:07}) with different \(\lambda \)-values. The initial weights are the same for all \(\lambda \)-values.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A histogram of the F1 scores for the same model but with different initial random weights with the mean and standard deviation for a 74 tanh - 104 sigmoid model (the same as in table \ref  {tab:07}). The only difference is that this is for a down-sampling ratio of \(\frac  {S_0}{S_1}\approx 1.3\).\relax }}{15}{figure.caption.41}\protected@file@percent }
\newlabel{fig:10}{{10}{15}{A histogram of the F1 scores for the same model but with different initial random weights with the mean and standard deviation for a 74 tanh - 104 sigmoid model (the same as in table \ref {tab:07}). The only difference is that this is for a down-sampling ratio of \(\frac {S_0}{S_1}\approx 1.3\).\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Regression analysis Neural Network}{15}{section*.42}\protected@file@percent }
\newlabel{sec:reg_NN}{{IV\tmspace  +\thinmuskip {.1667em}C}{15}{}{section*.42}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces The MSE and R2 error estimates for test section of the data for a learning rate of \(\eta = 10^{-4}\) and 500 epochs, except for tanh with 200 epochs. The design matrix used is for 5th order polynomial complexity.\relax }}{16}{table.caption.43}\protected@file@percent }
\newlabel{tab:08}{{VIII}{16}{The MSE and R2 error estimates for test section of the data for a learning rate of \(\eta = 10^{-4}\) and 500 epochs, except for tanh with 200 epochs. The design matrix used is for 5th order polynomial complexity.\relax }{table.caption.43}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IX}{\ignorespaces The MSE and R2 error estimates for test section of the data with different learning rates and 500 epochs for the elu activation function. The design matrix used is for 5th order polynomial complexity.\relax }}{16}{table.caption.44}\protected@file@percent }
\newlabel{tab:09}{{IX}{16}{The MSE and R2 error estimates for test section of the data with different learning rates and 500 epochs for the elu activation function. The design matrix used is for 5th order polynomial complexity.\relax }{table.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The R2 test and training error as a function of polynomial complexity for a 500 epochs, 0.0001 learning rate model with 50 neurons and elu in the hidden layer and relu as the activation function in the last layer. The error is with regards to the data set with noise.\relax }}{16}{figure.caption.45}\protected@file@percent }
\newlabel{fig:11}{{11}{16}{The R2 test and training error as a function of polynomial complexity for a 500 epochs, 0.0001 learning rate model with 50 neurons and elu in the hidden layer and relu as the activation function in the last layer. The error is with regards to the data set with noise.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The R2 test and training error as a function of polynomial complexity for a 500 epochs, 0.0001 learning rate model with 50 neurons and elu in the hidden layer and relu as the activation function in the last layer. The error is with regards to the Franke function without noise.\relax }}{16}{figure.caption.46}\protected@file@percent }
\newlabel{fig:12}{{12}{16}{The R2 test and training error as a function of polynomial complexity for a 500 epochs, 0.0001 learning rate model with 50 neurons and elu in the hidden layer and relu as the activation function in the last layer. The error is with regards to the Franke function without noise.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The best models for each neuron number for N=5 re-randomization of the initial weights. The model uses 500 epochs with a learning rate of 0.0001 and elu hidden activation function, and relu in the last layer.\relax }}{17}{figure.caption.47}\protected@file@percent }
\newlabel{fig:13}{{13}{17}{The best models for each neuron number for N=5 re-randomization of the initial weights. The model uses 500 epochs with a learning rate of 0.0001 and elu hidden activation function, and relu in the last layer.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The best models for each neuron number in the second layer for N=5 re-randomization of the initial weights. The model uses 500 epochs with a learning rate of 0.0001 and all elu activation functions except relu in the last layer. The first layer is the best layer from figure \ref  {fig:13}.\relax }}{17}{figure.caption.48}\protected@file@percent }
\newlabel{fig:14}{{14}{17}{The best models for each neuron number in the second layer for N=5 re-randomization of the initial weights. The model uses 500 epochs with a learning rate of 0.0001 and all elu activation functions except relu in the last layer. The first layer is the best layer from figure \ref {fig:13}.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The best models for each neuron number in the second layer for N=5 re-randomization of the initial weights. The model uses 500 epochs with a learning rate of 0.0001 and all elu activation functions except relu in the last layer. The first layer is the best layer from figure \ref  {fig:13} and second layer is the best from figure \ref  {fig:14}.\relax }}{17}{figure.caption.49}\protected@file@percent }
\newlabel{fig:15}{{15}{17}{The best models for each neuron number in the second layer for N=5 re-randomization of the initial weights. The model uses 500 epochs with a learning rate of 0.0001 and all elu activation functions except relu in the last layer. The first layer is the best layer from figure \ref {fig:13} and second layer is the best from figure \ref {fig:14}.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces A 3-dimensional plot of the best model from figure \ref  {fig:11} beside the actual Franke function.\relax }}{18}{figure.caption.50}\protected@file@percent }
\newlabel{fig:16}{{16}{18}{A 3-dimensional plot of the best model from figure \ref {fig:11} beside the actual Franke function.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{18}{section*.51}\protected@file@percent }
\newlabel{sec:Discussion}{{V}{18}{}{section*.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Credit card data}{18}{section*.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Logistic regression}{18}{section*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Neural Network}{19}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Comparison}{19}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Regression Neural network}{19}{section*.56}\protected@file@percent }
\citation{Jon_P1}
\citation{Jon_P1}
\citation{Jon_P1}
\citation{Jon_P1}
\citation{Jon_P1}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{20}{section*.57}\protected@file@percent }
\newlabel{sec:Conclusion}{{VI}{20}{}{section*.57}{}}
\@writefile{toc}{\appendix }
\@writefile{toc}{\contentsline {section}{\numberline {A}Tables NN}{21}{section*.58}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {X}{\ignorespaces The accuracy for a NN with sigmoid activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }}{21}{table.caption.59}\protected@file@percent }
\newlabel{tab:11}{{X}{21}{The accuracy for a NN with sigmoid activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }{table.caption.59}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XI}{\ignorespaces The F1 for a NN with sigmoid activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }}{21}{table.caption.60}\protected@file@percent }
\newlabel{tab:12}{{XI}{21}{The F1 for a NN with sigmoid activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }{table.caption.60}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XII}{\ignorespaces The accuracy for a NN with relu activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }}{21}{table.caption.61}\protected@file@percent }
\newlabel{tab:13}{{XII}{21}{The accuracy for a NN with relu activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }{table.caption.61}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XIII}{\ignorespaces The F1 for a NN with relu activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }}{21}{table.caption.62}\protected@file@percent }
\newlabel{tab:14}{{XIII}{21}{The F1 for a NN with relu activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }{table.caption.62}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XIV}{\ignorespaces The accuracy for a NN with elu activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }}{21}{table.caption.63}\protected@file@percent }
\newlabel{tab:15}{{XIV}{21}{The accuracy for a NN with elu activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }{table.caption.63}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XV}{\ignorespaces The F1 for a NN with elu activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }}{21}{table.caption.64}\protected@file@percent }
\newlabel{tab:16}{{XV}{21}{The F1 for a NN with elu activation function in the hidden layer with 64 nodes and softmax for the final activation function. The batch size used is 200.\relax }{table.caption.64}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Activation and cost functions}{21}{section*.65}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1}Cost function}{21}{section*.66}\protected@file@percent }
\bibdata{project2Notes,references}
\bibcite{MHJ_GD}{{1}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2}Activation functions}{22}{section*.67}\protected@file@percent }
\newlabel{sec:activation}{{B\tmspace  +\thinmuskip {.1667em}2}{22}{}{section*.67}{}}
\bibcite{MHJ_LogReg}{{2}{}{{}}{{}}}
\bibcite{MHJ_NN}{{3}{}{{}}{{}}}
\bibcite{Jon_P1}{{4}{}{{}}{{}}}
\bibcite{wiki:F1_score}{{5}{}{{}}{{}}}
\bibcite{yeh2009comparisons}{{6}{}{{}}{{}}}
\bibstyle{plain}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {}Referanser}{23}{section*.68}\protected@file@percent }
\newlabel{LastBibItem}{{6}{23}{}{section*.68}{}}
\newlabel{LastPage}{{}{23}{}{}{}}
