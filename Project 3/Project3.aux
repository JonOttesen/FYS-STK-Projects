\relax 
\providecommand\hyper@newdestlabel[2]{}
\bbl@beforestart
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{norsk}{}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{FYS-STK4155 â€“ Applied data analysis and machine learning\\ Project 3 - A game of snake using reinforced and supervised learning}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {abstract}{Sammendrag}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}\protected@file@percent }
\newlabel{sec:Introduction}{{I}{1}{}{section*.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Theory}{1}{section*.4}\protected@file@percent }
\newlabel{sec:Theory}{{II}{1}{}{section*.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Game of snake}{1}{section*.5}\protected@file@percent }
\newlabel{eq:01}{{II.1}{1}{}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Hamiltonian circuit}{1}{section*.6}\protected@file@percent }
\newlabel{sec:HC}{{II\tmspace  +\thinmuskip {.1667em}B}{1}{}{section*.6}{}}
\citation{Greedy_H}
\citation{Project2}
\newlabel{eq:02}{{II.2}{2}{}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Pertubated Hamiltonian cycle}{2}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Neural Network}{2}{section*.8}\protected@file@percent }
\citation{CS231}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}1D Neural network}{3}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Convolutional neural network}{3}{section*.10}\protected@file@percent }
\newlabel{eq:03}{{II.4}{3}{}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Convolution layer}{3}{section*.11}\protected@file@percent }
\newlabel{eq:06}{{II.6}{3}{}{equation.2.6}{}}
\newlabel{eq:04}{{II.7}{4}{}{equation.2.7}{}}
\newlabel{eq:05}{{II.8}{4}{}{equation.2.8}{}}
\newlabel{eq:07}{{II.9}{4}{}{equation.2.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Pooling layer}{4}{section*.12}\protected@file@percent }
\citation{RL}
\citation{wiki:Reinforcement_learning}
\newlabel{eq:08}{{II.14}{5}{}{equation.2.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Techniques etc}{5}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Supervised learning}{5}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {F}Reinforced learning}{5}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Q-learning}{6}{section*.16}\protected@file@percent }
\newlabel{eq:13}{{II.15}{6}{}{equation.2.15}{}}
\newlabel{eq:09}{{II.20}{6}{}{equation.2.20}{}}
\newlabel{eq:10}{{II.21}{7}{}{equation.2.21}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Q-learning\relax }}{7}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:01}{{1}{7}{Q-learning\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G}Model evaluation}{7}{section*.17}\protected@file@percent }
\newlabel{eq:11}{{II.26}{7}{}{equation.2.26}{}}
\newlabel{eq:12}{{II.27}{7}{}{equation.2.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{8}{section*.18}\protected@file@percent }
\newlabel{sec:Method}{{III}{8}{}{section*.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Code implementation}{8}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Snake}{8}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Convolutional neural network}{8}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Supervised Learning}{9}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Reinforced Learning}{9}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{10}{section*.24}\protected@file@percent }
\newlabel{sec:Results}{{IV}{10}{}{section*.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Hamiltonian circuit}{10}{section*.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The mean efficiency score for N = 100 runs for a 8x8 snake grid using the pertubated Hamiltonian path.\relax }}{10}{figure.caption.26}\protected@file@percent }
\newlabel{fig:01}{{1}{10}{The mean efficiency score for N = 100 runs for a 8x8 snake grid using the pertubated Hamiltonian path.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The mean efficiency score for N = 100 runs for a 8x8 snake grid using the regular Hamiltonian path.\relax }}{10}{figure.caption.27}\protected@file@percent }
\newlabel{fig:02}{{2}{10}{The mean efficiency score for N = 100 runs for a 8x8 snake grid using the regular Hamiltonian path.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The mean efficiency score for N = 100 runs for a 20x20 snake grid using the pertubated Hamiltonian path.\relax }}{11}{figure.caption.28}\protected@file@percent }
\newlabel{fig:03}{{3}{11}{The mean efficiency score for N = 100 runs for a 20x20 snake grid using the pertubated Hamiltonian path.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The mean efficiency score for N = 100 runs for a 20x20 snake grid using regular Hamiltonian path.\relax }}{11}{figure.caption.29}\protected@file@percent }
\newlabel{fig:04}{{4}{11}{The mean efficiency score for N = 100 runs for a 20x20 snake grid using regular Hamiltonian path.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Supervised Hamiltonian path}{11}{section*.30}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces The game scores for N=10 games on a 20x20 snake board, using model trained on the full Hamiltonian path.\relax }}{11}{table.caption.31}\protected@file@percent }
\newlabel{tab:01}{{I}{11}{The game scores for N=10 games on a 20x20 snake board, using model trained on the full Hamiltonian path.\relax }{table.caption.31}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The game scores for N=10 games on a 20x20 snake board, using model trained on the perturbed Hamiltonian path.\relax }}{11}{table.caption.32}\protected@file@percent }
\newlabel{tab:02}{{II}{11}{The game scores for N=10 games on a 20x20 snake board, using model trained on the perturbed Hamiltonian path.\relax }{table.caption.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces The game scores for N=10 games on a 8x8 snake board, using model trained on the full Hamiltonian path.\relax }}{11}{table.caption.33}\protected@file@percent }
\newlabel{tab:03}{{III}{11}{The game scores for N=10 games on a 8x8 snake board, using model trained on the full Hamiltonian path.\relax }{table.caption.33}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces The game scores for N=10 games on a 8x8 snake board, using model trained on the perturbed Hamiltonian path.\relax }}{11}{table.caption.34}\protected@file@percent }
\newlabel{tab:04}{{IV}{11}{The game scores for N=10 games on a 8x8 snake board, using model trained on the perturbed Hamiltonian path.\relax }{table.caption.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Reinforced Learning}{12}{section*.35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The mean game score of 250 games played on 5 re-trained models. The error bars represent the standard error between the N = 5 re-trained models, not the 250 games. This is done for three different learning rates in the CNN.\relax }}{12}{figure.caption.36}\protected@file@percent }
\newlabel{fig:05}{{5}{12}{The mean game score of 250 games played on 5 re-trained models. The error bars represent the standard error between the N = 5 re-trained models, not the 250 games. This is done for three different learning rates in the CNN.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The mean game score of 250 games played on 5 re-trained models. The plotted values are the maximum values achieved across the N = 5 re-trained models. This is done for three different learning rates in the CNN.\relax }}{12}{figure.caption.37}\protected@file@percent }
\newlabel{fig:06}{{6}{12}{The mean game score of 250 games played on 5 re-trained models. The plotted values are the maximum values achieved across the N = 5 re-trained models. This is done for three different learning rates in the CNN.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The mean game score of 250 games played on 5 re-trained models. The error bars represent the standard error between the N = 5 re-trained models. This is done for three different initial exploration factors in the reinforced learning algorithm.\relax }}{13}{figure.caption.38}\protected@file@percent }
\newlabel{fig:07}{{7}{13}{The mean game score of 250 games played on 5 re-trained models. The error bars represent the standard error between the N = 5 re-trained models. This is done for three different initial exploration factors in the reinforced learning algorithm.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The mean game score of 250 games played on 5 re-trained models. The plotted values are the maximum values achieved across the N = 5 re-trained models. This is done for three different initial exploration factors in the reinforced learning algorithm.\relax }}{13}{figure.caption.39}\protected@file@percent }
\newlabel{fig:08}{{8}{13}{The mean game score of 250 games played on 5 re-trained models. The plotted values are the maximum values achieved across the N = 5 re-trained models. This is done for three different initial exploration factors in the reinforced learning algorithm.\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The mean game score of 250 games played on 5 re-trained models. The error bars represent the standard error between the N = 5 re-trained models. This is done for four different movement penalties \(\beta \) in the reinforced learning algorithm.\relax }}{13}{figure.caption.40}\protected@file@percent }
\newlabel{fig:09}{{9}{13}{The mean game score of 250 games played on 5 re-trained models. The error bars represent the standard error between the N = 5 re-trained models. This is done for four different movement penalties \(\beta \) in the reinforced learning algorithm.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The mean game score of 250 games played on 5 re-trained models. The plotted values are the maximum values achieved across the N = 5 re-trained models. This is done for four different movement penalties \(\beta \) in the reinforced learning algorithm.\relax }}{14}{figure.caption.41}\protected@file@percent }
\newlabel{fig:10}{{10}{14}{The mean game score of 250 games played on 5 re-trained models. The plotted values are the maximum values achieved across the N = 5 re-trained models. This is done for four different movement penalties \(\beta \) in the reinforced learning algorithm.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The mean game score of 250 games played on 5 re-trained models. The error bars represent the standard error between the N = 5 re-trained models. This is done for four different discount factors \(\gamma \) in the reinforced learning algorithm.\relax }}{14}{figure.caption.42}\protected@file@percent }
\newlabel{fig:11}{{11}{14}{The mean game score of 250 games played on 5 re-trained models. The error bars represent the standard error between the N = 5 re-trained models. This is done for four different discount factors \(\gamma \) in the reinforced learning algorithm.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The mean game score of 250 games played on 5 re-trained models. The plotted values are the maximum values achieved across the N = 5 re-trained models. This is done for four different discount factor \(\gamma \) in the reinforced learning algorithm.\relax }}{14}{figure.caption.43}\protected@file@percent }
\newlabel{fig:12}{{12}{14}{The mean game score of 250 games played on 5 re-trained models. The plotted values are the maximum values achieved across the N = 5 re-trained models. This is done for four different discount factor \(\gamma \) in the reinforced learning algorithm.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces A histogram of the number of scored points for 250 games between a model trained on 250 episodes and one trained on 600 episodes.\relax }}{15}{figure.caption.44}\protected@file@percent }
\newlabel{fig:13}{{13}{15}{A histogram of the number of scored points for 250 games between a model trained on 250 episodes and one trained on 600 episodes.\relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{15}{section*.45}\protected@file@percent }
\newlabel{sec:Discussion}{{V}{15}{}{section*.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Supervised model}{15}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Reinforced learning}{16}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{16}{section*.48}\protected@file@percent }
\newlabel{sec:Conclusion}{{VI}{16}{}{section*.48}{}}
\bibdata{Project3Notes,references}
\bibcite{RL}{{1}{}{{}}{{}}}
\bibcite{CS231}{{2}{}{{}}{{}}}
\bibcite{Project2}{{3}{}{{}}{{}}}
\bibcite{Greedy_H}{{4}{}{{}}{{}}}
\bibcite{wiki:Reinforcement_learning}{{5}{}{{}}{{}}}
\bibstyle{plain}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {}Referanser}{17}{section*.49}\protected@file@percent }
\newlabel{LastBibItem}{{5}{17}{}{section*.49}{}}
\newlabel{LastPage}{{}{17}{}{}{}}
